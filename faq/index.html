
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, and other large language models.">
      
      
        <meta name="author" content="QubitPi (https://github.com/Qubitpi)">
      
      
        <link rel="canonical" href="https://ollama.qubitpi.org/faq/">
      
      
        <link rel="prev" href="../troubleshooting/">
      
      
        <link rel="next" href="../development/">
      
      
      <link rel="icon" href="../assets/logo/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.11">
    
    
      
        <title>FAQ - Ollama Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/css/customizations.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#faq" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Ollama Documentation" class="md-header__button md-logo" aria-label="Ollama Documentation" data-md-component="logo">
      
  <img src="../assets/logo/ollama.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ollama Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              FAQ
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/QubitPi/ollama" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ollama
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../examples/" class="md-tabs__link">
          
  
  
  Getting Started

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../api/" class="md-tabs__link">
          
  
  
  Reference

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../troubleshooting/" class="md-tabs__link">
          
  
  
  Resources

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Ollama Documentation" class="md-nav__button md-logo" aria-label="Ollama Documentation" data-md-component="logo">
      
  <img src="../assets/logo/ollama.png" alt="logo">

    </a>
    Ollama Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/QubitPi/ollama" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ollama
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../import/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Importing Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linux Documentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../windows/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Windows Documentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Docker Documentation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modelfile/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Modelfile Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../template/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Template
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../openai/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OpenAI Compatibility
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpu/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GPU
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Resources
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Resources
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../troubleshooting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Troubleshooting Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-can-i-upgrade-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      How can I upgrade Ollama?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-view-the-logs" class="md-nav__link">
    <span class="md-ellipsis">
      How can I view the logs?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-my-gpu-compatible-with-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      Is my GPU compatible with Ollama?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-specify-the-context-window-size" class="md-nav__link">
    <span class="md-ellipsis">
      How can I specify the context window size?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-tell-if-my-model-was-loaded-onto-the-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      How can I tell if my model was loaded onto the GPU?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-configure-ollama-server" class="md-nav__link">
    <span class="md-ellipsis">
      How do I configure Ollama server?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How do I configure Ollama server?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setting-environment-variables-on-mac" class="md-nav__link">
    <span class="md-ellipsis">
      Setting environment variables on Mac
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-environment-variables-on-linux" class="md-nav__link">
    <span class="md-ellipsis">
      Setting environment variables on Linux
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-environment-variables-on-windows" class="md-nav__link">
    <span class="md-ellipsis">
      Setting environment variables on Windows
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-use-ollama-behind-a-proxy" class="md-nav__link">
    <span class="md-ellipsis">
      How do I use Ollama behind a proxy?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How do I use Ollama behind a proxy?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-do-i-use-ollama-behind-a-proxy-in-docker" class="md-nav__link">
    <span class="md-ellipsis">
      How do I use Ollama behind a proxy in Docker?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#does-ollama-send-my-prompts-and-answers-back-to-ollamacom" class="md-nav__link">
    <span class="md-ellipsis">
      Does Ollama send my prompts and answers back to ollama.com?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-expose-ollama-on-my-network" class="md-nav__link">
    <span class="md-ellipsis">
      How can I expose Ollama on my network?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-use-ollama-with-a-proxy-server" class="md-nav__link">
    <span class="md-ellipsis">
      How can I use Ollama with a proxy server?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-use-ollama-with-ngrok" class="md-nav__link">
    <span class="md-ellipsis">
      How can I use Ollama with ngrok?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-use-ollama-with-cloudflare-tunnel" class="md-nav__link">
    <span class="md-ellipsis">
      How can I use Ollama with Cloudflare Tunnel?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-allow-additional-web-origins-to-access-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      How can I allow additional web origins to access Ollama?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#where-are-models-stored" class="md-nav__link">
    <span class="md-ellipsis">
      Where are models stored?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Where are models stored?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-do-i-set-them-to-a-different-location" class="md-nav__link">
    <span class="md-ellipsis">
      How do I set them to a different location?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-use-ollama-in-visual-studio-code" class="md-nav__link">
    <span class="md-ellipsis">
      How can I use Ollama in Visual Studio Code?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-use-ollama-with-gpu-acceleration-in-docker" class="md-nav__link">
    <span class="md-ellipsis">
      How do I use Ollama with GPU acceleration in Docker?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-is-networking-slow-in-wsl2-on-windows-10" class="md-nav__link">
    <span class="md-ellipsis">
      Why is networking slow in WSL2 on Windows 10?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-preload-a-model-into-ollama-to-get-faster-response-times" class="md-nav__link">
    <span class="md-ellipsis">
      How can I preload a model into Ollama to get faster response times?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately" class="md-nav__link">
    <span class="md-ellipsis">
      How do I keep a model loaded in memory or make it unload immediately?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-manage-the-maximum-number-of-requests-the-ollama-server-can-queue" class="md-nav__link">
    <span class="md-ellipsis">
      How do I manage the maximum number of requests the Ollama server can queue?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-does-ollama-handle-concurrent-requests" class="md-nav__link">
    <span class="md-ellipsis">
      How does Ollama handle concurrent requests?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-does-ollama-load-models-on-multiple-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      How does Ollama load models on multiple GPUs?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-enable-flash-attention" class="md-nav__link">
    <span class="md-ellipsis">
      How can I enable Flash Attention?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-set-the-quantization-type-for-the-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      How can I set the quantization type for the K/V cache?
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../development/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Development guide
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-can-i-upgrade-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      How can I upgrade Ollama?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-view-the-logs" class="md-nav__link">
    <span class="md-ellipsis">
      How can I view the logs?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#is-my-gpu-compatible-with-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      Is my GPU compatible with Ollama?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-specify-the-context-window-size" class="md-nav__link">
    <span class="md-ellipsis">
      How can I specify the context window size?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-tell-if-my-model-was-loaded-onto-the-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      How can I tell if my model was loaded onto the GPU?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-configure-ollama-server" class="md-nav__link">
    <span class="md-ellipsis">
      How do I configure Ollama server?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How do I configure Ollama server?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setting-environment-variables-on-mac" class="md-nav__link">
    <span class="md-ellipsis">
      Setting environment variables on Mac
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-environment-variables-on-linux" class="md-nav__link">
    <span class="md-ellipsis">
      Setting environment variables on Linux
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setting-environment-variables-on-windows" class="md-nav__link">
    <span class="md-ellipsis">
      Setting environment variables on Windows
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-use-ollama-behind-a-proxy" class="md-nav__link">
    <span class="md-ellipsis">
      How do I use Ollama behind a proxy?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How do I use Ollama behind a proxy?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-do-i-use-ollama-behind-a-proxy-in-docker" class="md-nav__link">
    <span class="md-ellipsis">
      How do I use Ollama behind a proxy in Docker?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#does-ollama-send-my-prompts-and-answers-back-to-ollamacom" class="md-nav__link">
    <span class="md-ellipsis">
      Does Ollama send my prompts and answers back to ollama.com?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-expose-ollama-on-my-network" class="md-nav__link">
    <span class="md-ellipsis">
      How can I expose Ollama on my network?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-use-ollama-with-a-proxy-server" class="md-nav__link">
    <span class="md-ellipsis">
      How can I use Ollama with a proxy server?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-use-ollama-with-ngrok" class="md-nav__link">
    <span class="md-ellipsis">
      How can I use Ollama with ngrok?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-use-ollama-with-cloudflare-tunnel" class="md-nav__link">
    <span class="md-ellipsis">
      How can I use Ollama with Cloudflare Tunnel?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-allow-additional-web-origins-to-access-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      How can I allow additional web origins to access Ollama?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#where-are-models-stored" class="md-nav__link">
    <span class="md-ellipsis">
      Where are models stored?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Where are models stored?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-do-i-set-them-to-a-different-location" class="md-nav__link">
    <span class="md-ellipsis">
      How do I set them to a different location?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-use-ollama-in-visual-studio-code" class="md-nav__link">
    <span class="md-ellipsis">
      How can I use Ollama in Visual Studio Code?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-use-ollama-with-gpu-acceleration-in-docker" class="md-nav__link">
    <span class="md-ellipsis">
      How do I use Ollama with GPU acceleration in Docker?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-is-networking-slow-in-wsl2-on-windows-10" class="md-nav__link">
    <span class="md-ellipsis">
      Why is networking slow in WSL2 on Windows 10?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-preload-a-model-into-ollama-to-get-faster-response-times" class="md-nav__link">
    <span class="md-ellipsis">
      How can I preload a model into Ollama to get faster response times?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately" class="md-nav__link">
    <span class="md-ellipsis">
      How do I keep a model loaded in memory or make it unload immediately?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-do-i-manage-the-maximum-number-of-requests-the-ollama-server-can-queue" class="md-nav__link">
    <span class="md-ellipsis">
      How do I manage the maximum number of requests the Ollama server can queue?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-does-ollama-handle-concurrent-requests" class="md-nav__link">
    <span class="md-ellipsis">
      How does Ollama handle concurrent requests?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-does-ollama-load-models-on-multiple-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      How does Ollama load models on multiple GPUs?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-enable-flash-attention" class="md-nav__link">
    <span class="md-ellipsis">
      How can I enable Flash Attention?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-can-i-set-the-quantization-type-for-the-kv-cache" class="md-nav__link">
    <span class="md-ellipsis">
      How can I set the quantization type for the K/V cache?
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/QubitPi/ollama/edit/master/docs/faq.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/QubitPi/ollama/raw/master/docs/faq.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="faq"><a class="toclink" href="#faq">FAQ</a></h1>
<h2 id="how-can-i-upgrade-ollama"><a class="toclink" href="#how-can-i-upgrade-ollama">How can I upgrade Ollama?</a></h2>
<p>Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click "Restart to update" to apply the update. Updates can also be installed by downloading the latest version <a href="https://ollama.com/download/">manually</a>.</p>
<p>On Linux, re-run the install script:</p>
<div class="highlight"><pre><span></span><code>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://ollama.com/install.sh<span class="w"> </span><span class="p">|</span><span class="w"> </span>sh
</code></pre></div>
<h2 id="how-can-i-view-the-logs"><a class="toclink" href="#how-can-i-view-the-logs">How can I view the logs?</a></h2>
<p>Review the <a href="../troubleshooting/">Troubleshooting</a> docs for more about using logs.</p>
<h2 id="is-my-gpu-compatible-with-ollama"><a class="toclink" href="#is-my-gpu-compatible-with-ollama">Is my GPU compatible with Ollama?</a></h2>
<p>Please refer to the <a href="../gpu/">GPU docs</a>.</p>
<h2 id="how-can-i-specify-the-context-window-size"><a class="toclink" href="#how-can-i-specify-the-context-window-size">How can I specify the context window size?</a></h2>
<p>By default, Ollama uses a context window size of 2048 tokens. </p>
<p>This can be overridden with the <code>OLLAMA_CONTEXT_LENGTH</code> environment variable. For example, to set the default context window to 8K, use: </p>
<div class="highlight"><pre><span></span><code><span class="nv">OLLAMA_CONTEXT_LENGTH</span><span class="o">=</span><span class="m">8192</span><span class="w"> </span>ollama<span class="w"> </span>serve
</code></pre></div>
<p>To change this when using <code>ollama run</code>, use <code>/set parameter</code>:</p>
<div class="highlight"><pre><span></span><code>/set<span class="w"> </span>parameter<span class="w"> </span>num_ctx<span class="w"> </span><span class="m">4096</span>
</code></pre></div>
<p>When using the API, specify the <code>num_ctx</code> parameter:</p>
<div class="highlight"><pre><span></span><code>curl<span class="w"> </span>http://localhost:11434/api/generate<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;model&quot;: &quot;llama3.2&quot;,</span>
<span class="s1">  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,</span>
<span class="s1">  &quot;options&quot;: {</span>
<span class="s1">    &quot;num_ctx&quot;: 4096</span>
<span class="s1">  }</span>
<span class="s1">}&#39;</span>
</code></pre></div>
<h2 id="how-can-i-tell-if-my-model-was-loaded-onto-the-gpu"><a class="toclink" href="#how-can-i-tell-if-my-model-was-loaded-onto-the-gpu">How can I tell if my model was loaded onto the GPU?</a></h2>
<p>Use the <code>ollama ps</code> command to see what models are currently loaded into memory.</p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>ps
</code></pre></div>
<blockquote>
<p><strong>Output</strong>:</p>
<div class="highlight"><pre><span></span><code>NAME          ID              SIZE    PROCESSOR   UNTIL
llama3:70b    bcfb190ca3a7    42 GB   100% GPU    4 minutes from now
</code></pre></div>
</blockquote>
<p>The <code>Processor</code> column will show which memory the model was loaded in to:
* <code>100% GPU</code> means the model was loaded entirely into the GPU
* <code>100% CPU</code> means the model was loaded entirely in system memory
* <code>48%/52% CPU/GPU</code> means the model was loaded partially onto both the GPU and into system memory</p>
<h2 id="how-do-i-configure-ollama-server"><a class="toclink" href="#how-do-i-configure-ollama-server">How do I configure Ollama server?</a></h2>
<p>Ollama server can be configured with environment variables.</p>
<h3 id="setting-environment-variables-on-mac"><a class="toclink" href="#setting-environment-variables-on-mac">Setting environment variables on Mac</a></h3>
<p>If Ollama is run as a macOS application, environment variables should be set using <code>launchctl</code>:</p>
<ol>
<li>
<p>For each environment variable, call <code>launchctl setenv</code>.</p>
<div class="highlight"><pre><span></span><code>launchctl<span class="w"> </span>setenv<span class="w"> </span>OLLAMA_HOST<span class="w"> </span><span class="s2">&quot;0.0.0.0:11434&quot;</span>
</code></pre></div>
</li>
<li>
<p>Restart Ollama application.</p>
</li>
</ol>
<h3 id="setting-environment-variables-on-linux"><a class="toclink" href="#setting-environment-variables-on-linux">Setting environment variables on Linux</a></h3>
<p>If Ollama is run as a systemd service, environment variables should be set using <code>systemctl</code>:</p>
<ol>
<li>
<p>Edit the systemd service by calling <code>systemctl edit ollama.service</code>. This will open an editor.</p>
</li>
<li>
<p>For each environment variable, add a line <code>Environment</code> under section <code>[Service]</code>:</p>
<div class="highlight"><pre><span></span><code><span class="k">[Service]</span>
<span class="na">Environment</span><span class="o">=</span><span class="s">&quot;OLLAMA_HOST=0.0.0.0:11434&quot;</span>
</code></pre></div>
</li>
<li>
<p>Save and exit.</p>
</li>
<li>
<p>Reload <code>systemd</code> and restart Ollama:</p>
</li>
</ol>
<div class="highlight"><pre><span></span><code>systemctl<span class="w"> </span>daemon-reload
systemctl<span class="w"> </span>restart<span class="w"> </span>ollama
</code></pre></div>
<h3 id="setting-environment-variables-on-windows"><a class="toclink" href="#setting-environment-variables-on-windows">Setting environment variables on Windows</a></h3>
<p>On Windows, Ollama inherits your user and system environment variables.</p>
<ol>
<li>
<p>First Quit Ollama by clicking on it in the task bar.</p>
</li>
<li>
<p>Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for <em>environment variables</em>.</p>
</li>
<li>
<p>Click on <em>Edit environment variables for your account</em>.</p>
</li>
<li>
<p>Edit or create a new variable for your user account for <code>OLLAMA_HOST</code>, <code>OLLAMA_MODELS</code>, etc.</p>
</li>
<li>
<p>Click OK/Apply to save.</p>
</li>
<li>
<p>Start the Ollama application from the Windows Start menu.</p>
</li>
</ol>
<h2 id="how-do-i-use-ollama-behind-a-proxy"><a class="toclink" href="#how-do-i-use-ollama-behind-a-proxy">How do I use Ollama behind a proxy?</a></h2>
<p>Ollama pulls models from the Internet and may require a proxy server to access the models. Use <code>HTTPS_PROXY</code> to redirect outbound requests through the proxy. Ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.</p>
<blockquote>
<p>[!NOTE]
Avoid setting <code>HTTP_PROXY</code>. Ollama does not use HTTP for model pulls, only HTTPS. Setting <code>HTTP_PROXY</code> may interrupt client connections to the server.</p>
</blockquote>
<h3 id="how-do-i-use-ollama-behind-a-proxy-in-docker"><a class="toclink" href="#how-do-i-use-ollama-behind-a-proxy-in-docker">How do I use Ollama behind a proxy in Docker?</a></h3>
<p>The Ollama Docker container image can be configured to use a proxy by passing <code>-e HTTPS_PROXY=https://proxy.example.com</code> when starting the container.</p>
<p>Alternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on <a href="https://docs.docker.com/desktop/settings/mac/#proxies">macOS</a>, <a href="https://docs.docker.com/desktop/settings/windows/#proxies">Windows</a>, and <a href="https://docs.docker.com/desktop/settings/linux/#proxies">Linux</a>, and Docker <a href="https://docs.docker.com/config/daemon/systemd/#httphttps-proxy">daemon with systemd</a>.</p>
<p>Ensure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.</p>
<div class="highlight"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">ollama/ollama</span>
<span class="k">COPY</span><span class="w"> </span>my-ca.pem<span class="w"> </span>/usr/local/share/ca-certificates/my-ca.crt
<span class="k">RUN</span><span class="w"> </span>update-ca-certificates
</code></pre></div>
<p>Build and run this image:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>ollama-with-ca<span class="w"> </span>.
docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>-e<span class="w"> </span><span class="nv">HTTPS_PROXY</span><span class="o">=</span>https://my.proxy.example.com<span class="w"> </span>-p<span class="w"> </span><span class="m">11434</span>:11434<span class="w"> </span>ollama-with-ca
</code></pre></div>
<h2 id="does-ollama-send-my-prompts-and-answers-back-to-ollamacom"><a class="toclink" href="#does-ollama-send-my-prompts-and-answers-back-to-ollamacom">Does Ollama send my prompts and answers back to ollama.com?</a></h2>
<p>No. Ollama runs locally, and conversation data does not leave your machine.</p>
<h2 id="how-can-i-expose-ollama-on-my-network"><a class="toclink" href="#how-can-i-expose-ollama-on-my-network">How can I expose Ollama on my network?</a></h2>
<p>Ollama binds 127.0.0.1 port 11434 by default. Change the bind address with the <code>OLLAMA_HOST</code> environment variable.</p>
<p>Refer to the section <a href="#how-do-i-configure-ollama-server">above</a> for how to set environment variables on your platform.</p>
<h2 id="how-can-i-use-ollama-with-a-proxy-server"><a class="toclink" href="#how-can-i-use-ollama-with-a-proxy-server">How can I use Ollama with a proxy server?</a></h2>
<p>Ollama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:</p>
<div class="highlight"><pre><span></span><code><span class="k">server</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kn">listen</span><span class="w"> </span><span class="mi">80</span><span class="p">;</span>
<span class="w">    </span><span class="kn">server_name</span><span class="w"> </span><span class="s">example.com</span><span class="p">;</span><span class="w">  </span><span class="c1"># Replace with your domain or IP</span>
<span class="w">    </span><span class="kn">location</span><span class="w"> </span><span class="s">/</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kn">proxy_pass</span><span class="w"> </span><span class="s">http://localhost:11434</span><span class="p">;</span>
<span class="w">        </span><span class="kn">proxy_set_header</span><span class="w"> </span><span class="s">Host</span><span class="w"> </span><span class="n">localhost</span><span class="p">:</span><span class="mi">11434</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<h2 id="how-can-i-use-ollama-with-ngrok"><a class="toclink" href="#how-can-i-use-ollama-with-ngrok">How can I use Ollama with ngrok?</a></h2>
<p>Ollama can be accessed using a range of tools for tunneling tools. For example with Ngrok:</p>
<div class="highlight"><pre><span></span><code>ngrok<span class="w"> </span>http<span class="w"> </span><span class="m">11434</span><span class="w"> </span>--host-header<span class="o">=</span><span class="s2">&quot;localhost:11434&quot;</span>
</code></pre></div>
<h2 id="how-can-i-use-ollama-with-cloudflare-tunnel"><a class="toclink" href="#how-can-i-use-ollama-with-cloudflare-tunnel">How can I use Ollama with Cloudflare Tunnel?</a></h2>
<p>To use Ollama with Cloudflare Tunnel, use the <code>--url</code> and <code>--http-host-header</code> flags:</p>
<div class="highlight"><pre><span></span><code>cloudflared<span class="w"> </span>tunnel<span class="w"> </span>--url<span class="w"> </span>http://localhost:11434<span class="w"> </span>--http-host-header<span class="o">=</span><span class="s2">&quot;localhost:11434&quot;</span>
</code></pre></div>
<h2 id="how-can-i-allow-additional-web-origins-to-access-ollama"><a class="toclink" href="#how-can-i-allow-additional-web-origins-to-access-ollama">How can I allow additional web origins to access Ollama?</a></h2>
<p>Ollama allows cross-origin requests from <code>127.0.0.1</code> and <code>0.0.0.0</code> by default. Additional origins can be configured with <code>OLLAMA_ORIGINS</code>.</p>
<p>For browser extensions, you'll need to explicitly allow the extension's origin pattern. Set <code>OLLAMA_ORIGINS</code> to include <code>chrome-extension://*</code>, <code>moz-extension://*</code>, and <code>safari-web-extension://*</code> if you wish to allow all browser extensions access, or specific extensions as needed:</p>
<div class="highlight"><pre><span></span><code># Allow all Chrome, Firefox, and Safari extensions
OLLAMA_ORIGINS=chrome-extension://*,moz-extension://*,safari-web-extension://* ollama serve
</code></pre></div>
<p>Refer to the section <a href="#how-do-i-configure-ollama-server">above</a> for how to set environment variables on your platform.</p>
<h2 id="where-are-models-stored"><a class="toclink" href="#where-are-models-stored">Where are models stored?</a></h2>
<ul>
<li>macOS: <code>~/.ollama/models</code></li>
<li>Linux: <code>/usr/share/ollama/.ollama/models</code></li>
<li>Windows: <code>C:\Users\%username%\.ollama\models</code></li>
</ul>
<h3 id="how-do-i-set-them-to-a-different-location"><a class="toclink" href="#how-do-i-set-them-to-a-different-location">How do I set them to a different location?</a></h3>
<p>If a different directory needs to be used, set the environment variable <code>OLLAMA_MODELS</code> to the chosen directory.</p>
<blockquote>
<p>Note: on Linux using the standard installer, the <code>ollama</code> user needs read and write access to the specified directory. To assign the directory to the <code>ollama</code> user run <code>sudo chown -R ollama:ollama &lt;directory&gt;</code>.</p>
</blockquote>
<p>Refer to the section <a href="#how-do-i-configure-ollama-server">above</a> for how to set environment variables on your platform.</p>
<h2 id="how-can-i-use-ollama-in-visual-studio-code"><a class="toclink" href="#how-can-i-use-ollama-in-visual-studio-code">How can I use Ollama in Visual Studio Code?</a></h2>
<p>There is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of <a href="https://github.com/ollama/ollama#extensions--plugins">extensions &amp; plugins</a> at the bottom of the main repository readme.</p>
<h2 id="how-do-i-use-ollama-with-gpu-acceleration-in-docker"><a class="toclink" href="#how-do-i-use-ollama-with-gpu-acceleration-in-docker">How do I use Ollama with GPU acceleration in Docker?</a></h2>
<p>The Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the <a href="https://github.com/NVIDIA/nvidia-container-toolkit">nvidia-container-toolkit</a>. See <a href="https://hub.docker.com/r/ollama/ollama">ollama/ollama</a> for more details.</p>
<p>GPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.</p>
<h2 id="why-is-networking-slow-in-wsl2-on-windows-10"><a class="toclink" href="#why-is-networking-slow-in-wsl2-on-windows-10">Why is networking slow in WSL2 on Windows 10?</a></h2>
<p>This can impact both installing Ollama, as well as downloading models.</p>
<p>Open <code>Control Panel &gt; Networking and Internet &gt; View network status and tasks</code> and click on <code>Change adapter settings</code> on the left panel. Find the <code>vEthernel (WSL)</code> adapter, right click and select <code>Properties</code>.
Click on <code>Configure</code> and open the <code>Advanced</code> tab. Search through each of the properties until you find <code>Large Send Offload Version 2 (IPv4)</code> and <code>Large Send Offload Version 2 (IPv6)</code>. <em>Disable</em> both of these
properties.</p>
<h2 id="how-can-i-preload-a-model-into-ollama-to-get-faster-response-times"><a class="toclink" href="#how-can-i-preload-a-model-into-ollama-to-get-faster-response-times">How can I preload a model into Ollama to get faster response times?</a></h2>
<p>If you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the <code>/api/generate</code> and <code>/api/chat</code> API endpoints.</p>
<p>To preload the mistral model using the generate endpoint, use:</p>
<div class="highlight"><pre><span></span><code>curl<span class="w"> </span>http://localhost:11434/api/generate<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;model&quot;: &quot;mistral&quot;}&#39;</span>
</code></pre></div>
<p>To use the chat completions endpoint, use:</p>
<div class="highlight"><pre><span></span><code>curl<span class="w"> </span>http://localhost:11434/api/chat<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;model&quot;: &quot;mistral&quot;}&#39;</span>
</code></pre></div>
<p>To preload a model using the CLI, use the command:</p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>run<span class="w"> </span>llama3.2<span class="w"> </span><span class="s2">&quot;&quot;</span>
</code></pre></div>
<h2 id="how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately"><a class="toclink" href="#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately">How do I keep a model loaded in memory or make it unload immediately?</a></h2>
<p>By default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you're making numerous requests to the LLM. If you want to immediately unload a model from memory, use the <code>ollama stop</code> command:</p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>stop<span class="w"> </span>llama3.2
</code></pre></div>
<p>If you're using the API, use the <code>keep_alive</code> parameter with the <code>/api/generate</code> and <code>/api/chat</code> endpoints to set the amount of time that a model stays in memory. The <code>keep_alive</code> parameter can be set to:
* a duration string (such as "10m" or "24h")
* a number in seconds (such as 3600)
* any negative number which will keep the model loaded in memory (e.g. -1 or "-1m")
* '0' which will unload the model immediately after generating a response</p>
<p>For example, to preload a model and leave it in memory use:</p>
<div class="highlight"><pre><span></span><code>curl<span class="w"> </span>http://localhost:11434/api/generate<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;model&quot;: &quot;llama3.2&quot;, &quot;keep_alive&quot;: -1}&#39;</span>
</code></pre></div>
<p>To unload the model and free up memory use:</p>
<div class="highlight"><pre><span></span><code>curl<span class="w"> </span>http://localhost:11434/api/generate<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;model&quot;: &quot;llama3.2&quot;, &quot;keep_alive&quot;: 0}&#39;</span>
</code></pre></div>
<p>Alternatively, you can change the amount of time all models are loaded into memory by setting the <code>OLLAMA_KEEP_ALIVE</code> environment variable when starting the Ollama server. The <code>OLLAMA_KEEP_ALIVE</code> variable uses the same parameter types as the <code>keep_alive</code> parameter types mentioned above. Refer to the section explaining <a href="#how-do-i-configure-ollama-server">how to configure the Ollama server</a> to correctly set the environment variable.</p>
<p>The <code>keep_alive</code> API parameter with the <code>/api/generate</code> and <code>/api/chat</code> API endpoints will override the <code>OLLAMA_KEEP_ALIVE</code> setting.</p>
<h2 id="how-do-i-manage-the-maximum-number-of-requests-the-ollama-server-can-queue"><a class="toclink" href="#how-do-i-manage-the-maximum-number-of-requests-the-ollama-server-can-queue">How do I manage the maximum number of requests the Ollama server can queue?</a></h2>
<p>If too many requests are sent to the server, it will respond with a 503 error indicating the server is overloaded.  You can adjust how many requests may be queue by setting <code>OLLAMA_MAX_QUEUE</code>.</p>
<h2 id="how-does-ollama-handle-concurrent-requests"><a class="toclink" href="#how-does-ollama-handle-concurrent-requests">How does Ollama handle concurrent requests?</a></h2>
<p>Ollama supports two levels of concurrent processing.  If your system has sufficient available memory (system memory when using CPU inference, or VRAM for GPU inference) then multiple models can be loaded at the same time.  For a given model, if there is sufficient available memory when the model is loaded, it is configured to allow parallel request processing.</p>
<p>If there is insufficient available memory to load a new model request while one or more models are already loaded, all new requests will be queued until the new model can be loaded.  As prior models become idle, one or more will be unloaded to make room for the new model.  Queued requests will be processed in order.  When using GPU inference new models must be able to completely fit in VRAM to allow concurrent model loads.</p>
<p>Parallel request processing for a given model results in increasing the context size by the number of parallel requests.  For example, a 2K context with 4 parallel requests will result in an 8K context and additional memory allocation.</p>
<p>The following server settings may be used to adjust how Ollama handles concurrent requests on most platforms:</p>
<ul>
<li><code>OLLAMA_MAX_LOADED_MODELS</code> - The maximum number of models that can be loaded concurrently provided they fit in available memory.  The default is 3 * the number of GPUs or 3 for CPU inference.</li>
<li><code>OLLAMA_NUM_PARALLEL</code> - The maximum number of parallel requests each model will process at the same time.  The default will auto-select either 4 or 1 based on available memory.</li>
<li><code>OLLAMA_MAX_QUEUE</code> - The maximum number of requests Ollama will queue when busy before rejecting additional requests. The default is 512</li>
</ul>
<p>Note: Windows with Radeon GPUs currently default to 1 model maximum due to limitations in ROCm v5.7 for available VRAM reporting.  Once ROCm v6.2 is available, Windows Radeon will follow the defaults above.  You may enable concurrent model loads on Radeon on Windows, but ensure you don't load more models than will fit into your GPUs VRAM.</p>
<h2 id="how-does-ollama-load-models-on-multiple-gpus"><a class="toclink" href="#how-does-ollama-load-models-on-multiple-gpus">How does Ollama load models on multiple GPUs?</a></h2>
<p>When loading a new model, Ollama evaluates the required VRAM for the model against what is currently available.  If the model will entirely fit on any single GPU, Ollama will load the model on that GPU.  This typically provides the best performance as it reduces the amount of data transferring across the PCI bus during inference.  If the model does not fit entirely on one GPU, then it will be spread across all the available GPUs.</p>
<h2 id="how-can-i-enable-flash-attention"><a class="toclink" href="#how-can-i-enable-flash-attention">How can I enable Flash Attention?</a></h2>
<p>Flash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows.  To enable Flash Attention, set the <code>OLLAMA_FLASH_ATTENTION</code> environment variable to <code>1</code> when starting the Ollama server.</p>
<h2 id="how-can-i-set-the-quantization-type-for-the-kv-cache"><a class="toclink" href="#how-can-i-set-the-quantization-type-for-the-kv-cache">How can I set the quantization type for the K/V cache?</a></h2>
<p>The K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.</p>
<p>To use quantized K/V cache with Ollama you can set the following environment variable:</p>
<ul>
<li><code>OLLAMA_KV_CACHE_TYPE</code> - The quantization type for the K/V cache.  Default is <code>f16</code>.</li>
</ul>
<blockquote>
<p>Note: Currently this is a global option - meaning all models will run with the specified quantization type.</p>
</blockquote>
<p>The currently available K/V cache quantization types are:</p>
<ul>
<li><code>f16</code> - high precision and memory usage (default).</li>
<li><code>q8_0</code> - 8-bit quantization, uses approximately 1/2 the memory of <code>f16</code> with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).</li>
<li><code>q4_0</code> - 4-bit quantization, uses approximately 1/4 the memory of <code>f16</code> with a small-medium loss in precision that may be more noticeable at higher context sizes.</li>
</ul>
<p>How much the cache quantization impacts the model's response quality will depend on the model and the task.  Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.</p>
<p>You may need to experiment with different quantization types to find the best balance between memory usage and quality.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      <p>&copy <a href="https://github.com/ollama"><em>Ollama Organization</em></a><br/><span>This project is licensed under the MIT license.</span></p>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.top", "navigation.expand", "navigation.instant", "content.action.edit", "content.action.view", "content.code.annotate"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>