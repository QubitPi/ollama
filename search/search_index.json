{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quickstart</li> <li>Examples</li> <li>Importing models</li> <li>Linux Documentation</li> <li>Windows Documentation</li> <li>Docker Documentation</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference</li> <li>Modelfile Reference</li> <li>Template</li> <li>OpenAI Compatibility</li> <li>GPU</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Troubleshooting Guide</li> <li>FAQ</li> <li>Development guide</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"api/#endpoints","title":"Endpoints","text":"<ul> <li>Generate a completion</li> <li>Generate a chat completion</li> <li>Create a Model</li> <li>List Local Models</li> <li>Show Model Information</li> <li>Copy a Model</li> <li>Delete a Model</li> <li>Pull a Model</li> <li>Push a Model</li> <li>Generate Embeddings</li> <li>List Running Models</li> <li>Version</li> </ul>"},{"location":"api/#conventions","title":"Conventions","text":""},{"location":"api/#model-names","title":"Model names","text":"<p>Model names follow a <code>model:tag</code> format, where <code>model</code> can have an optional namespace such as <code>example/model</code>. Some examples are <code>orca-mini:3b-q8_0</code> and <code>llama3:70b</code>. The tag is optional and, if not provided, will default to <code>latest</code>. The tag is used to identify a specific version.</p>"},{"location":"api/#durations","title":"Durations","text":"<p>All durations are returned in nanoseconds.</p>"},{"location":"api/#streaming-responses","title":"Streaming responses","text":"<p>Certain endpoints stream responses as JSON objects. Streaming can be disabled by providing <code>{\"stream\": false}</code> for these endpoints.</p>"},{"location":"api/#generate-a-completion","title":"Generate a completion","text":"<pre><code>POST /api/generate\n</code></pre> <p>Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.</p>"},{"location":"api/#parameters","title":"Parameters","text":"<ul> <li><code>model</code>: (required) the model name</li> <li><code>prompt</code>: the prompt to generate a response for</li> <li><code>suffix</code>: the text after the model response</li> <li><code>images</code>: (optional) a list of base64-encoded images (for multimodal models such as <code>llava</code>)</li> <li><code>think</code>: (for thinking models) should the model think before responding?</li> </ul> <p>Advanced parameters (optional):</p> <ul> <li><code>format</code>: the format to return a response in. Format can be <code>json</code> or a JSON schema</li> <li><code>options</code>: additional model parameters listed in the documentation for the Modelfile such as <code>temperature</code></li> <li><code>system</code>: system message to (overrides what is defined in the <code>Modelfile</code>)</li> <li><code>template</code>: the prompt template to use (overrides what is defined in the <code>Modelfile</code>)</li> <li><code>stream</code>: if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li> <li><code>raw</code>: if <code>true</code> no formatting will be applied to the prompt. You may choose to use the <code>raw</code> parameter if you are specifying a full templated prompt in your request to the API</li> <li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li> <li><code>context</code> (deprecated): the context parameter returned from a previous request to <code>/generate</code>, this can be used to keep a short conversational memory</li> </ul>"},{"location":"api/#structured-outputs","title":"Structured outputs","text":"<p>Structured outputs are supported by providing a JSON schema in the <code>format</code> parameter. The model will generate a response that matches the schema. See the structured outputs example below.</p>"},{"location":"api/#json-mode","title":"JSON mode","text":"<p>Enable JSON mode by setting the <code>format</code> parameter to <code>json</code>. This will structure the response as a valid JSON object. See the JSON mode example below.</p> <p>Tip</p> <p>It's important to instruct the model to use JSON in the <code>prompt</code>. Otherwise, the model may generate large amounts whitespace.</p>"},{"location":"api/#examples","title":"Examples","text":""},{"location":"api/#generate-request-streaming","title":"Generate request (Streaming)","text":""},{"location":"api/#request","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\n</code></pre>"},{"location":"api/#response","title":"Response","text":"<p>A stream of JSON objects is returned:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"response\": \"The\",\n  \"done\": false\n}\n</code></pre> <p>The final response in the stream also includes additional data about the generation:</p> <ul> <li><code>total_duration</code>: time spent generating the response</li> <li><code>load_duration</code>: time spent in nanoseconds loading the model</li> <li><code>prompt_eval_count</code>: number of tokens in the prompt</li> <li><code>prompt_eval_duration</code>: time spent in nanoseconds evaluating the prompt</li> <li><code>eval_count</code>: number of tokens in the response</li> <li><code>eval_duration</code>: time in nanoseconds spent generating the response</li> <li><code>context</code>: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory</li> <li><code>response</code>: empty if the response was streamed, if not streamed, this will contain the full response</li> </ul> <p>To calculate how fast the response is generated in tokens per second (token/s), divide <code>eval_count</code> / <code>eval_duration</code> * <code>10^9</code>.</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 10706818083,\n  \"load_duration\": 6338219291,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 130079000,\n  \"eval_count\": 259,\n  \"eval_duration\": 4232710000\n}\n</code></pre>"},{"location":"api/#request-no-streaming","title":"Request (No streaming)","text":""},{"location":"api/#request_1","title":"Request","text":"<p>A response can be received in one reply when streaming is off.</p> <pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n</code></pre>"},{"location":"api/#response_1","title":"Response","text":"<p>If <code>stream</code> is set to <code>false</code>, the response will be a single JSON object:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"The sky is blue because it is the color of the sky.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 5043500667,\n  \"load_duration\": 5025959,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 325953000,\n  \"eval_count\": 290,\n  \"eval_duration\": 4709213000\n}\n</code></pre>"},{"location":"api/#request-with-suffix","title":"Request (with suffix)","text":""},{"location":"api/#request_2","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama:code\",\n  \"prompt\": \"def compute_gcd(a, b):\",\n  \"suffix\": \"    return result\",\n  \"options\": {\n    \"temperature\": 0\n  },\n  \"stream\": false\n}'\n</code></pre>"},{"location":"api/#response_2","title":"Response","text":"<pre><code>{\n  \"model\": \"codellama:code\",\n  \"created_at\": \"2024-07-22T20:47:51.147561Z\",\n  \"response\": \"\\n  if a == 0:\\n    return b\\n  else:\\n    return compute_gcd(b % a, a)\\n\\ndef compute_lcm(a, b):\\n  result = (a * b) / compute_gcd(a, b)\\n\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"context\": [...],\n  \"total_duration\": 1162761250,\n  \"load_duration\": 6683708,\n  \"prompt_eval_count\": 17,\n  \"prompt_eval_duration\": 201222000,\n  \"eval_count\": 63,\n  \"eval_duration\": 953997000\n}\n</code></pre>"},{"location":"api/#request-structured-outputs","title":"Request (Structured outputs)","text":""},{"location":"api/#request_3","title":"Request","text":"<pre><code>curl -X POST http://localhost:11434/api/generate -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Ollama is 22 years old and is busy saving the world. Respond using JSON\",\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"available\": {\n        \"type\": \"boolean\"\n      }\n    },\n    \"required\": [\n      \"age\",\n      \"available\"\n    ]\n  }\n}'\n</code></pre>"},{"location":"api/#response_3","title":"Response","text":"<pre><code>{\n  \"model\": \"llama3.1:8b\",\n  \"created_at\": \"2024-12-06T00:48:09.983619Z\",\n  \"response\": \"{\\n  \\\"age\\\": 22,\\n  \\\"available\\\": true\\n}\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"context\": [1, 2, 3],\n  \"total_duration\": 1075509083,\n  \"load_duration\": 567678166,\n  \"prompt_eval_count\": 28,\n  \"prompt_eval_duration\": 236000000,\n  \"eval_count\": 16,\n  \"eval_duration\": 269000000\n}\n</code></pre>"},{"location":"api/#request-json-mode","title":"Request (JSON mode)","text":"<p>Tip</p> <p>When <code>format</code> is set to <code>json</code>, the output will always be a well-formed JSON object. It's important to also instruct the model to respond in JSON.</p>"},{"location":"api/#request_4","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"What color is the sky at different times of the day? Respond using JSON\",\n  \"format\": \"json\",\n  \"stream\": false\n}'\n</code></pre>"},{"location":"api/#response_4","title":"Response","text":"<pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-11-09T21:07:55.186497Z\",\n  \"response\": \"{\\n\\\"morning\\\": {\\n\\\"color\\\": \\\"blue\\\"\\n},\\n\\\"noon\\\": {\\n\\\"color\\\": \\\"blue-gray\\\"\\n},\\n\\\"afternoon\\\": {\\n\\\"color\\\": \\\"warm gray\\\"\\n},\\n\\\"evening\\\": {\\n\\\"color\\\": \\\"orange\\\"\\n}\\n}\\n\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 4648158584,\n  \"load_duration\": 4071084,\n  \"prompt_eval_count\": 36,\n  \"prompt_eval_duration\": 439038000,\n  \"eval_count\": 180,\n  \"eval_duration\": 4196918000\n}\n</code></pre> <p>The value of <code>response</code> will be a string containing JSON similar to:</p> <pre><code>{\n  \"morning\": {\n    \"color\": \"blue\"\n  },\n  \"noon\": {\n    \"color\": \"blue-gray\"\n  },\n  \"afternoon\": {\n    \"color\": \"warm gray\"\n  },\n  \"evening\": {\n    \"color\": \"orange\"\n  }\n}\n</code></pre>"},{"location":"api/#request-with-images","title":"Request (with images)","text":"<p>To submit images to multimodal models such as <code>llava</code> or <code>bakllava</code>, provide a list of base64-encoded <code>images</code>:</p>"},{"location":"api/#request_5","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"stream\": false,\n  \"images\": [\"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\"]\n}'\n</code></pre>"},{"location":"api/#response_5","title":"Response","text":"<pre><code>{\n  \"model\": \"llava\",\n  \"created_at\": \"2023-11-03T15:36:02.583064Z\",\n  \"response\": \"A happy cartoon character, which is cute and cheerful.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 2938432250,\n  \"load_duration\": 2559292,\n  \"prompt_eval_count\": 1,\n  \"prompt_eval_duration\": 2195557000,\n  \"eval_count\": 44,\n  \"eval_duration\": 736432000\n}\n</code></pre>"},{"location":"api/#request-raw-mode","title":"Request (Raw Mode)","text":"<p>In some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the <code>raw</code> parameter to disable templating. Also note that raw mode will not return a context.</p>"},{"location":"api/#request_6","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"[INST] why is the sky blue? [/INST]\",\n  \"raw\": true,\n  \"stream\": false\n}'\n</code></pre>"},{"location":"api/#request-reproducible-outputs","title":"Request (Reproducible outputs)","text":"<p>For reproducible outputs, set <code>seed</code> to a number:</p>"},{"location":"api/#request_7","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"seed\": 123\n  }\n}'\n</code></pre>"},{"location":"api/#response_6","title":"Response","text":"<pre><code>{\n  \"model\": \"mistral\",\n  \"created_at\": \"2023-11-03T15:36:02.583064Z\",\n  \"response\": \" The sky appears blue because of a phenomenon called Rayleigh scattering.\",\n  \"done\": true,\n  \"total_duration\": 8493852375,\n  \"load_duration\": 6589624375,\n  \"prompt_eval_count\": 14,\n  \"prompt_eval_duration\": 119039000,\n  \"eval_count\": 110,\n  \"eval_duration\": 1779061000\n}\n</code></pre>"},{"location":"api/#generate-request-with-options","title":"Generate request (With options)","text":"<p>If you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the <code>options</code> parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.</p>"},{"location":"api/#request_8","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false,\n  \"options\": {\n    \"num_keep\": 5,\n    \"seed\": 42,\n    \"num_predict\": 100,\n    \"top_k\": 20,\n    \"top_p\": 0.9,\n    \"min_p\": 0.0,\n    \"typical_p\": 0.7,\n    \"repeat_last_n\": 33,\n    \"temperature\": 0.8,\n    \"repeat_penalty\": 1.2,\n    \"presence_penalty\": 1.5,\n    \"frequency_penalty\": 1.0,\n    \"penalize_newline\": true,\n    \"stop\": [\"\\n\", \"user:\"],\n    \"numa\": false,\n    \"num_ctx\": 1024,\n    \"num_batch\": 2,\n    \"num_gpu\": 1,\n    \"main_gpu\": 0,\n    \"use_mmap\": true,\n    \"num_thread\": 8\n  }\n}'\n</code></pre>"},{"location":"api/#response_7","title":"Response","text":"<pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"The sky is blue because it is the color of the sky.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 4935886791,\n  \"load_duration\": 534986708,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 107345000,\n  \"eval_count\": 237,\n  \"eval_duration\": 4289432000\n}\n</code></pre>"},{"location":"api/#load-a-model","title":"Load a model","text":"<p>If an empty prompt is provided, the model will be loaded into memory.</p>"},{"location":"api/#request_9","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\"\n}'\n</code></pre>"},{"location":"api/#response_8","title":"Response","text":"<p>A single JSON object is returned:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-18T19:52:07.071755Z\",\n  \"response\": \"\",\n  \"done\": true\n}\n</code></pre>"},{"location":"api/#unload-a-model","title":"Unload a model","text":"<p>If an empty prompt is provided and the <code>keep_alive</code> parameter is set to <code>0</code>, a model will be unloaded from memory.</p>"},{"location":"api/#request_10","title":"Request","text":"<pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"keep_alive\": 0\n}'\n</code></pre>"},{"location":"api/#response_9","title":"Response","text":"<p>A single JSON object is returned:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2024-09-12T03:54:03.516566Z\",\n  \"response\": \"\",\n  \"done\": true,\n  \"done_reason\": \"unload\"\n}\n</code></pre>"},{"location":"api/#generate-a-chat-completion","title":"Generate a chat completion","text":"<pre><code>POST /api/chat\n</code></pre> <p>Generate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using <code>\"stream\": false</code>. The final response object will include statistics and additional data from the request.</p>"},{"location":"api/#parameters_1","title":"Parameters","text":"<ul> <li><code>model</code>: (required) the model name</li> <li><code>messages</code>: the messages of the chat, this can be used to keep a chat memory</li> <li><code>tools</code>: list of tools in JSON for the model to use if supported</li> <li><code>think</code>: (for thinking models) should the model think before responding?</li> </ul> <p>The <code>message</code> object has the following fields:</p> <ul> <li><code>role</code>: the role of the message, either <code>system</code>, <code>user</code>, <code>assistant</code>, or <code>tool</code></li> <li><code>content</code>: the content of the message</li> <li><code>thinking</code>: (for thinking models) the model's thinking process</li> <li><code>images</code> (optional): a list of images to include in the message (for multimodal models such as <code>llava</code>)</li> <li><code>tool_calls</code> (optional): a list of tools in JSON that the model wants to use</li> </ul> <p>Advanced parameters (optional):</p> <ul> <li><code>format</code>: the format to return a response in. Format can be <code>json</code> or a JSON schema. </li> <li><code>options</code>: additional model parameters listed in the documentation for the Modelfile such as <code>temperature</code></li> <li><code>stream</code>: if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li> <li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li> </ul>"},{"location":"api/#structured-outputs_1","title":"Structured outputs","text":"<p>Structured outputs are supported by providing a JSON schema in the <code>format</code> parameter. The model will generate a response that matches the schema. See the Chat request (Structured outputs) example below.</p>"},{"location":"api/#examples_1","title":"Examples","text":""},{"location":"api/#chat-request-streaming","title":"Chat Request (Streaming)","text":""},{"location":"api/#request_11","title":"Request","text":"<p>Send a chat message with a streaming response.</p> <pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    }\n  ]\n}'\n</code></pre>"},{"location":"api/#response_10","title":"Response","text":"<p>A stream of JSON objects is returned:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The\",\n    \"images\": null\n  },\n  \"done\": false\n}\n</code></pre> <p>Final response:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done\": true,\n  \"total_duration\": 4883583458,\n  \"load_duration\": 1334875,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 342546000,\n  \"eval_count\": 282,\n  \"eval_duration\": 4535599000\n}\n</code></pre>"},{"location":"api/#chat-request-no-streaming","title":"Chat request (No streaming)","text":""},{"location":"api/#request_12","title":"Request","text":"<pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    }\n  ],\n  \"stream\": false\n}'\n</code></pre>"},{"location":"api/#response_11","title":"Response","text":"<pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-12T14:13:43.416799Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Hello! How are you today?\"\n  },\n  \"done\": true,\n  \"total_duration\": 5191566416,\n  \"load_duration\": 2154458,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 383809000,\n  \"eval_count\": 298,\n  \"eval_duration\": 4799921000\n}\n</code></pre>"},{"location":"api/#chat-request-structured-outputs","title":"Chat request (Structured outputs)","text":""},{"location":"api/#request_13","title":"Request","text":"<pre><code>curl -X POST http://localhost:11434/api/chat -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability.\"}],\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"available\": {\n        \"type\": \"boolean\"\n      }\n    },\n    \"required\": [\n      \"age\",\n      \"available\"\n    ]\n  },\n  \"options\": {\n    \"temperature\": 0\n  }\n}'\n</code></pre>"},{"location":"api/#response_12","title":"Response","text":"<pre><code>{\n  \"model\": \"llama3.1\",\n  \"created_at\": \"2024-12-06T00:46:58.265747Z\",\n  \"message\": { \"role\": \"assistant\", \"content\": \"{\\\"age\\\": 22, \\\"available\\\": false}\" },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 2254970291,\n  \"load_duration\": 574751416,\n  \"prompt_eval_count\": 34,\n  \"prompt_eval_duration\": 1502000000,\n  \"eval_count\": 12,\n  \"eval_duration\": 175000000\n}\n</code></pre>"},{"location":"api/#chat-request-with-history","title":"Chat request (With History)","text":"<p>Send a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.</p>"},{"location":"api/#request_14","title":"Request","text":"<pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"due to rayleigh scattering.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"how is that different than mie scattering?\"\n    }\n  ]\n}'\n</code></pre>"},{"location":"api/#response_13","title":"Response","text":"<p>A stream of JSON objects is returned:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The\"\n  },\n  \"done\": false\n}\n</code></pre> <p>Final response:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"done\": true,\n  \"total_duration\": 8113331500,\n  \"load_duration\": 6396458,\n  \"prompt_eval_count\": 61,\n  \"prompt_eval_duration\": 398801000,\n  \"eval_count\": 468,\n  \"eval_duration\": 7701267000\n}\n</code></pre>"},{"location":"api/#chat-request-with-images","title":"Chat request (with images)","text":""},{"location":"api/#request_15","title":"Request","text":"<p>Send a chat message with images. The images should be provided as an array, with the individual images encoded in Base64.</p> <pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llava\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"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\"]\n    }\n  ]\n}'\n</code></pre>"},{"location":"api/#response_14","title":"Response","text":"<pre><code>{\n  \"model\": \"llava\",\n  \"created_at\": \"2023-12-13T22:42:50.203334Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \" The image features a cute, little pig with an angry facial expression. It's wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.\",\n    \"images\": null\n  },\n  \"done\": true,\n  \"total_duration\": 1668506709,\n  \"load_duration\": 1986209,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 359682000,\n  \"eval_count\": 83,\n  \"eval_duration\": 1303285000\n}\n</code></pre>"},{"location":"api/#chat-request-reproducible-outputs","title":"Chat request (Reproducible outputs)","text":""},{"location":"api/#request_16","title":"Request","text":"<pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello!\"\n    }\n  ],\n  \"options\": {\n    \"seed\": 101,\n    \"temperature\": 0\n  }\n}'\n</code></pre>"},{"location":"api/#response_15","title":"Response","text":"<pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-12T14:13:43.416799Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Hello! How are you today?\"\n  },\n  \"done\": true,\n  \"total_duration\": 5191566416,\n  \"load_duration\": 2154458,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 383809000,\n  \"eval_count\": 298,\n  \"eval_duration\": 4799921000\n}\n</code></pre>"},{"location":"api/#chat-request-with-tools","title":"Chat request (with tools)","text":""},{"location":"api/#request_17","title":"Request","text":"<pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the weather today in Paris?\"\n    }\n  ],\n  \"stream\": false,\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather for a location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The location to get the weather for, e.g. San Francisco, CA\"\n            },\n            \"format\": {\n              \"type\": \"string\",\n              \"description\": \"The format to return the weather in, e.g. 'celsius' or 'fahrenheit'\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\", \"format\"]\n        }\n      }\n    }\n  ]\n}'\n</code></pre>"},{"location":"api/#response_16","title":"Response","text":"<pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2024-07-22T20:33:28.123648Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\",\n    \"tool_calls\": [\n      {\n        \"function\": {\n          \"name\": \"get_current_weather\",\n          \"arguments\": {\n            \"format\": \"celsius\",\n            \"location\": \"Paris, FR\"\n          }\n        }\n      }\n    ]\n  },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 885095291,\n  \"load_duration\": 3753500,\n  \"prompt_eval_count\": 122,\n  \"prompt_eval_duration\": 328493000,\n  \"eval_count\": 33,\n  \"eval_duration\": 552222000\n}\n</code></pre>"},{"location":"api/#load-a-model_1","title":"Load a model","text":"<p>If the messages array is empty, the model will be loaded into memory.</p>"},{"location":"api/#request_18","title":"Request","text":"<pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": []\n}'\n</code></pre>"},{"location":"api/#response_17","title":"Response","text":"<pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\":\"2024-09-12T21:17:29.110811Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done_reason\": \"load\",\n  \"done\": true\n}\n</code></pre>"},{"location":"api/#unload-a-model_1","title":"Unload a model","text":"<p>If the messages array is empty and the <code>keep_alive</code> parameter is set to <code>0</code>, a model will be unloaded from memory.</p>"},{"location":"api/#request_19","title":"Request","text":"<pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [],\n  \"keep_alive\": 0\n}'\n</code></pre>"},{"location":"api/#response_18","title":"Response","text":"<p>A single JSON object is returned:</p> <pre><code>{\n  \"model\": \"llama3.2\",\n  \"created_at\":\"2024-09-12T21:33:17.547535Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done_reason\": \"unload\",\n  \"done\": true\n}\n</code></pre>"},{"location":"api/#create-a-model","title":"Create a Model","text":"<pre><code>POST /api/create\n</code></pre> <p>Create a model from:  * another model;  * a safetensors directory; or  * a GGUF file.</p> <p>If you are creating a model from a safetensors directory or from a GGUF file, you must create a blob for each of the files and then use the file name and SHA256 digest associated with each blob in the <code>files</code> field.</p>"},{"location":"api/#parameters_2","title":"Parameters","text":"<ul> <li><code>model</code>: name of the model to create</li> <li><code>from</code>: (optional) name of an existing model to create the new model from</li> <li><code>files</code>: (optional) a dictionary of file names to SHA256 digests of blobs to create the model from</li> <li><code>adapters</code>: (optional) a dictionary of file names to SHA256 digests of blobs for LORA adapters</li> <li><code>template</code>: (optional) the prompt template for the model</li> <li><code>license</code>: (optional) a string or list of strings containing the license or licenses for the model</li> <li><code>system</code>: (optional) a string containing the system prompt for the model</li> <li><code>parameters</code>: (optional) a dictionary of parameters for the model (see Modelfile for a list of parameters)</li> <li><code>messages</code>: (optional) a list of message objects used to create a conversation</li> <li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li> <li><code>quantize</code> (optional): quantize a non-quantized (e.g. float16) model</li> </ul>"},{"location":"api/#quantization-types","title":"Quantization types","text":"Type Recommended q4_K_M * q4_K_S q8_0 *"},{"location":"api/#examples_2","title":"Examples","text":""},{"location":"api/#create-a-new-model","title":"Create a new model","text":"<p>Create a new model from an existing model.</p>"},{"location":"api/#request_20","title":"Request","text":"<pre><code>curl http://localhost:11434/api/create -d '{\n  \"model\": \"mario\",\n  \"from\": \"llama3.2\",\n  \"system\": \"You are Mario from Super Mario Bros.\"\n}'\n</code></pre>"},{"location":"api/#response_19","title":"Response","text":"<p>A stream of JSON objects is returned:</p> <pre><code>{\"status\":\"reading model metadata\"}\n{\"status\":\"creating system layer\"}\n{\"status\":\"using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2\"}\n{\"status\":\"using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b\"}\n{\"status\":\"using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d\"}\n{\"status\":\"using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988\"}\n{\"status\":\"using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9\"}\n{\"status\":\"writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42\"}\n{\"status\":\"writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n</code></pre>"},{"location":"api/#quantize-a-model","title":"Quantize a model","text":"<p>Quantize a non-quantized model.</p>"},{"location":"api/#request_21","title":"Request","text":"<pre><code>curl http://localhost:11434/api/create -d '{\n  \"model\": \"llama3.2:quantized\",\n  \"from\": \"llama3.2:3b-instruct-fp16\",\n  \"quantize\": \"q4_K_M\"\n}'\n</code></pre>"},{"location":"api/#response_20","title":"Response","text":"<p>A stream of JSON objects is returned:</p> <pre><code>{\"status\":\"quantizing F16 model to Q4_K_M\",\"digest\":\"0\",\"total\":6433687776,\"completed\":12302}\n{\"status\":\"quantizing F16 model to Q4_K_M\",\"digest\":\"0\",\"total\":6433687776,\"completed\":6433687552}\n{\"status\":\"verifying conversion\"}\n{\"status\":\"creating new layer sha256:fb7f4f211b89c6c4928ff4ddb73db9f9c0cfca3e000c3e40d6cf27ddc6ca72eb\"}\n{\"status\":\"using existing layer sha256:966de95ca8a62200913e3f8bfbf84c8494536f1b94b49166851e76644e966396\"}\n{\"status\":\"using existing layer sha256:fcc5a6bec9daf9b561a68827b67ab6088e1dba9d1fa2a50d7bbcc8384e0a265d\"}\n{\"status\":\"using existing layer sha256:a70ff7e570d97baaf4e62ac6e6ad9975e04caa6d900d3742d37698494479e0cd\"}\n{\"status\":\"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n</code></pre>"},{"location":"api/#create-a-model-from-gguf","title":"Create a model from GGUF","text":"<p>Create a model from a GGUF file. The <code>files</code> parameter should be filled out with the file name and SHA256 digest of the GGUF file you wish to use. Use /api/blobs/:digest to push the GGUF file to the server before calling this API.</p>"},{"location":"api/#request_22","title":"Request","text":"<pre><code>curl http://localhost:11434/api/create -d '{\n  \"model\": \"my-gguf-model\",\n  \"files\": {\n    \"test.gguf\": \"sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3\"\n  }\n}'\n</code></pre>"},{"location":"api/#response_21","title":"Response","text":"<p>A stream of JSON objects is returned:</p> <pre><code>{\"status\":\"parsing GGUF\"}\n{\"status\":\"using existing layer sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n</code></pre>"},{"location":"api/#create-a-model-from-a-safetensors-directory","title":"Create a model from a Safetensors directory","text":"<p>The <code>files</code> parameter should include a dictionary of files for the safetensors model which includes the file names and SHA256 digest of each file. Use /api/blobs/:digest to first push each of the files to the server before calling this API. Files will remain in the cache until the Ollama server is restarted.</p>"},{"location":"api/#request_23","title":"Request","text":"<pre><code>curl http://localhost:11434/api/create -d '{\n  \"model\": \"fred\",\n  \"files\": {\n    \"config.json\": \"sha256:dd3443e529fb2290423a0c65c2d633e67b419d273f170259e27297219828e389\",\n    \"generation_config.json\": \"sha256:88effbb63300dbbc7390143fbbdd9d9fa50587b37e8bfd16c8c90d4970a74a36\",\n    \"special_tokens_map.json\": \"sha256:b7455f0e8f00539108837bfa586c4fbf424e31f8717819a6798be74bef813d05\",\n    \"tokenizer.json\": \"sha256:bbc1904d35169c542dffbe1f7589a5994ec7426d9e5b609d07bab876f32e97ab\",\n    \"tokenizer_config.json\": \"sha256:24e8a6dc2547164b7002e3125f10b415105644fcf02bf9ad8b674c87b1eaaed6\",\n    \"model.safetensors\": \"sha256:1ff795ff6a07e6a68085d206fb84417da2f083f68391c2843cd2b8ac6df8538f\"\n  }\n}'\n</code></pre>"},{"location":"api/#response_22","title":"Response","text":"<p>A stream of JSON objects is returned:</p> <pre><code>{\"status\":\"converting model\"}\n{\"status\":\"creating new layer sha256:05ca5b813af4a53d2c2922933936e398958855c44ee534858fcfd830940618b6\"}\n{\"status\":\"using autodetected template llama3-instruct\"}\n{\"status\":\"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n</code></pre>"},{"location":"api/#check-if-a-blob-exists","title":"Check if a Blob Exists","text":"<pre><code>HEAD /api/blobs/:digest\n</code></pre> <p>Ensures that the file blob (Binary Large Object) used with create a model exists on the server. This checks your Ollama server and not ollama.com.</p>"},{"location":"api/#query-parameters","title":"Query Parameters","text":"<ul> <li><code>digest</code>: the SHA256 digest of the blob</li> </ul>"},{"location":"api/#examples_3","title":"Examples","text":""},{"location":"api/#request_24","title":"Request","text":"<pre><code>curl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n</code></pre>"},{"location":"api/#response_23","title":"Response","text":"<p>Return 200 OK if the blob exists, 404 Not Found if it does not.</p>"},{"location":"api/#push-a-blob","title":"Push a Blob","text":"<pre><code>POST /api/blobs/:digest\n</code></pre> <p>Push a file to the Ollama server to create a \"blob\" (Binary Large Object).</p>"},{"location":"api/#query-parameters_1","title":"Query Parameters","text":"<ul> <li><code>digest</code>: the expected SHA256 digest of the file</li> </ul>"},{"location":"api/#examples_4","title":"Examples","text":""},{"location":"api/#request_25","title":"Request","text":"<pre><code>curl -T model.gguf -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n</code></pre>"},{"location":"api/#response_24","title":"Response","text":"<p>Return 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.</p>"},{"location":"api/#list-local-models","title":"List Local Models","text":"<pre><code>GET /api/tags\n</code></pre> <p>List models that are available locally.</p>"},{"location":"api/#examples_5","title":"Examples","text":""},{"location":"api/#request_26","title":"Request","text":"<pre><code>curl http://localhost:11434/api/tags\n</code></pre>"},{"location":"api/#response_25","title":"Response","text":"<p>A single JSON object will be returned.</p> <pre><code>{\n  \"models\": [\n    {\n      \"name\": \"deepseek-r1:latest\",\n      \"model\": \"deepseek-r1:latest\",\n      \"modified_at\": \"2025-05-10T08:06:48.639712648-07:00\",\n      \"size\": 4683075271,\n      \"digest\": \"0a8c266910232fd3291e71e5ba1e058cc5af9d411192cf88b6d30e92b6e73163\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"qwen2\",\n        \"families\": [\n          \"qwen2\"\n        ],\n        \"parameter_size\": \"7.6B\",\n        \"quantization_level\": \"Q4_K_M\"\n      }\n    },\n    {\n      \"name\": \"llama3.2:latest\",\n      \"model\": \"llama3.2:latest\",\n      \"modified_at\": \"2025-05-04T17:37:44.706015396-07:00\",\n      \"size\": 2019393189,\n      \"digest\": \"a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": [\n          \"llama\"\n        ],\n        \"parameter_size\": \"3.2B\",\n        \"quantization_level\": \"Q4_K_M\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"api/#show-model-information","title":"Show Model Information","text":"<pre><code>POST /api/show\n</code></pre> <p>Show information about a model including details, modelfile, template, parameters, license, system prompt.</p>"},{"location":"api/#parameters_3","title":"Parameters","text":"<ul> <li><code>model</code>: name of the model to show</li> <li><code>verbose</code>: (optional) if set to <code>true</code>, returns full data for verbose response fields</li> </ul>"},{"location":"api/#examples_6","title":"Examples","text":""},{"location":"api/#request_27","title":"Request","text":"<pre><code>curl http://localhost:11434/api/show -d '{\n  \"model\": \"llava\"\n}'\n</code></pre>"},{"location":"api/#response_26","title":"Response","text":"<pre><code>{\n  \"modelfile\": \"# Modelfile generated by \\\"ollama show\\\"\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM llava:latest\\n\\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\\nTEMPLATE \\\"\\\"\\\"{{ .System }}\\nUSER: {{ .Prompt }}\\nASSISTANT: \\\"\\\"\\\"\\nPARAMETER num_ctx 4096\\nPARAMETER stop \\\"\\u003c/s\\u003e\\\"\\nPARAMETER stop \\\"USER:\\\"\\nPARAMETER stop \\\"ASSISTANT:\\\"\",\n  \"parameters\": \"num_keep                       24\\nstop                           \\\"&lt;|start_header_id|&gt;\\\"\\nstop                           \\\"&lt;|end_header_id|&gt;\\\"\\nstop                           \\\"&lt;|eot_id|&gt;\\\"\",\n  \"template\": \"{{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n{{ .System }}&lt;|eot_id|&gt;{{ end }}{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n{{ .Response }}&lt;|eot_id|&gt;\",\n  \"details\": {\n    \"parent_model\": \"\",\n    \"format\": \"gguf\",\n    \"family\": \"llama\",\n    \"families\": [\n      \"llama\"\n    ],\n    \"parameter_size\": \"8.0B\",\n    \"quantization_level\": \"Q4_0\"\n  },\n  \"model_info\": {\n    \"general.architecture\": \"llama\",\n    \"general.file_type\": 2,\n    \"general.parameter_count\": 8030261248,\n    \"general.quantization_version\": 2,\n    \"llama.attention.head_count\": 32,\n    \"llama.attention.head_count_kv\": 8,\n    \"llama.attention.layer_norm_rms_epsilon\": 0.00001,\n    \"llama.block_count\": 32,\n    \"llama.context_length\": 8192,\n    \"llama.embedding_length\": 4096,\n    \"llama.feed_forward_length\": 14336,\n    \"llama.rope.dimension_count\": 128,\n    \"llama.rope.freq_base\": 500000,\n    \"llama.vocab_size\": 128256,\n    \"tokenizer.ggml.bos_token_id\": 128000,\n    \"tokenizer.ggml.eos_token_id\": 128009,\n    \"tokenizer.ggml.merges\": [],            // populates if `verbose=true`\n    \"tokenizer.ggml.model\": \"gpt2\",\n    \"tokenizer.ggml.pre\": \"llama-bpe\",\n    \"tokenizer.ggml.token_type\": [],        // populates if `verbose=true`\n    \"tokenizer.ggml.tokens\": []             // populates if `verbose=true`\n  },\n  \"capabilities\": [\n    \"completion\",\n    \"vision\"\n  ],\n}\n</code></pre>"},{"location":"api/#copy-a-model","title":"Copy a Model","text":"<pre><code>POST /api/copy\n</code></pre> <p>Copy a model. Creates a model with another name from an existing model.</p>"},{"location":"api/#examples_7","title":"Examples","text":""},{"location":"api/#request_28","title":"Request","text":"<pre><code>curl http://localhost:11434/api/copy -d '{\n  \"source\": \"llama3.2\",\n  \"destination\": \"llama3-backup\"\n}'\n</code></pre>"},{"location":"api/#response_27","title":"Response","text":"<p>Returns a 200 OK if successful, or a 404 Not Found if the source model doesn't exist.</p>"},{"location":"api/#delete-a-model","title":"Delete a Model","text":"<pre><code>DELETE /api/delete\n</code></pre> <p>Delete a model and its data.</p>"},{"location":"api/#parameters_4","title":"Parameters","text":"<ul> <li><code>model</code>: model name to delete</li> </ul>"},{"location":"api/#examples_8","title":"Examples","text":""},{"location":"api/#request_29","title":"Request","text":"<pre><code>curl -X DELETE http://localhost:11434/api/delete -d '{\n  \"model\": \"llama3:13b\"\n}'\n</code></pre>"},{"location":"api/#response_28","title":"Response","text":"<p>Returns a 200 OK if successful, 404 Not Found if the model to be deleted doesn't exist.</p>"},{"location":"api/#pull-a-model","title":"Pull a Model","text":"<pre><code>POST /api/pull\n</code></pre> <p>Download a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.</p>"},{"location":"api/#parameters_5","title":"Parameters","text":"<ul> <li><code>model</code>: name of the model to pull</li> <li><code>insecure</code>: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development.</li> <li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li> </ul>"},{"location":"api/#examples_9","title":"Examples","text":""},{"location":"api/#request_30","title":"Request","text":"<pre><code>curl http://localhost:11434/api/pull -d '{\n  \"model\": \"llama3.2\"\n}'\n</code></pre>"},{"location":"api/#response_29","title":"Response","text":"<p>If <code>stream</code> is not specified, or set to <code>true</code>, a stream of JSON objects is returned:</p> <p>The first object is the manifest:</p> <pre><code>{\n  \"status\": \"pulling manifest\"\n}\n</code></pre> <p>Then there is a series of downloading responses. Until any of the download is completed, the <code>completed</code> key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.</p> <pre><code>{\n  \"status\": \"downloading digestname\",\n  \"digest\": \"digestname\",\n  \"total\": 2142590208,\n  \"completed\": 241970\n}\n</code></pre> <p>After all the files are downloaded, the final responses are:</p> <pre><code>{\n    \"status\": \"verifying sha256 digest\"\n}\n{\n    \"status\": \"writing manifest\"\n}\n{\n    \"status\": \"removing any unused layers\"\n}\n{\n    \"status\": \"success\"\n}\n</code></pre> <p>if <code>stream</code> is set to false, then the response is a single JSON object:</p> <pre><code>{\n  \"status\": \"success\"\n}\n</code></pre>"},{"location":"api/#push-a-model","title":"Push a Model","text":"<pre><code>POST /api/push\n</code></pre> <p>Upload a model to a model library. Requires registering for ollama.ai and adding a public key first.</p>"},{"location":"api/#parameters_6","title":"Parameters","text":"<ul> <li><code>model</code>: name of the model to push in the form of <code>&lt;namespace&gt;/&lt;model&gt;:&lt;tag&gt;</code></li> <li><code>insecure</code>: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development.</li> <li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li> </ul>"},{"location":"api/#examples_10","title":"Examples","text":""},{"location":"api/#request_31","title":"Request","text":"<pre><code>curl http://localhost:11434/api/push -d '{\n  \"model\": \"mattw/pygmalion:latest\"\n}'\n</code></pre>"},{"location":"api/#response_30","title":"Response","text":"<p>If <code>stream</code> is not specified, or set to <code>true</code>, a stream of JSON objects is returned:</p> <pre><code>{ \"status\": \"retrieving manifest\" }\n</code></pre> <p>and then:</p> <pre><code>{\n  \"status\": \"starting upload\",\n  \"digest\": \"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\",\n  \"total\": 1928429856\n}\n</code></pre> <p>Then there is a series of uploading responses:</p> <pre><code>{\n  \"status\": \"starting upload\",\n  \"digest\": \"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\",\n  \"total\": 1928429856\n}\n</code></pre> <p>Finally, when the upload is complete:</p> <pre><code>{\"status\":\"pushing manifest\"}\n{\"status\":\"success\"}\n</code></pre> <p>If <code>stream</code> is set to <code>false</code>, then the response is a single JSON object:</p> <pre><code>{ \"status\": \"success\" }\n</code></pre>"},{"location":"api/#generate-embeddings","title":"Generate Embeddings","text":"<pre><code>POST /api/embed\n</code></pre> <p>Generate embeddings from a model</p>"},{"location":"api/#parameters_7","title":"Parameters","text":"<ul> <li><code>model</code>: name of model to generate embeddings from</li> <li><code>input</code>: text or list of text to generate embeddings for</li> </ul> <p>Advanced parameters:</p> <ul> <li><code>truncate</code>: truncates the end of each input to fit within context length. Returns error if <code>false</code> and context length is exceeded. Defaults to <code>true</code></li> <li><code>options</code>: additional model parameters listed in the documentation for the Modelfile such as <code>temperature</code></li> <li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li> </ul>"},{"location":"api/#examples_11","title":"Examples","text":""},{"location":"api/#request_32","title":"Request","text":"<pre><code>curl http://localhost:11434/api/embed -d '{\n  \"model\": \"all-minilm\",\n  \"input\": \"Why is the sky blue?\"\n}'\n</code></pre>"},{"location":"api/#response_31","title":"Response","text":"<pre><code>{\n  \"model\": \"all-minilm\",\n  \"embeddings\": [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ]],\n  \"total_duration\": 14143917,\n  \"load_duration\": 1019500,\n  \"prompt_eval_count\": 8\n}\n</code></pre>"},{"location":"api/#request-multiple-input","title":"Request (Multiple input)","text":"<pre><code>curl http://localhost:11434/api/embed -d '{\n  \"model\": \"all-minilm\",\n  \"input\": [\"Why is the sky blue?\", \"Why is the grass green?\"]\n}'\n</code></pre>"},{"location":"api/#response_32","title":"Response","text":"<pre><code>{\n  \"model\": \"all-minilm\",\n  \"embeddings\": [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ],[\n    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,\n    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481\n  ]]\n}\n</code></pre>"},{"location":"api/#list-running-models","title":"List Running Models","text":"<pre><code>GET /api/ps\n</code></pre> <p>List models that are currently loaded into memory.</p>"},{"location":"api/#examples_12","title":"Examples","text":""},{"location":"api/#request_33","title":"Request","text":"<pre><code>curl http://localhost:11434/api/ps\n</code></pre>"},{"location":"api/#response_33","title":"Response","text":"<p>A single JSON object will be returned.</p> <pre><code>{\n  \"models\": [\n    {\n      \"name\": \"mistral:latest\",\n      \"model\": \"mistral:latest\",\n      \"size\": 5137025024,\n      \"digest\": \"2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": [\n          \"llama\"\n        ],\n        \"parameter_size\": \"7.2B\",\n        \"quantization_level\": \"Q4_0\"\n      },\n      \"expires_at\": \"2024-06-04T14:38:31.83753-07:00\",\n      \"size_vram\": 5137025024\n    }\n  ]\n}\n</code></pre>"},{"location":"api/#generate-embedding","title":"Generate Embedding","text":"<p>Note: this endpoint has been superseded by <code>/api/embed</code></p> <pre><code>POST /api/embeddings\n</code></pre> <p>Generate embeddings from a model</p>"},{"location":"api/#parameters_8","title":"Parameters","text":"<ul> <li><code>model</code>: name of model to generate embeddings from</li> <li><code>prompt</code>: text to generate embeddings for</li> </ul> <p>Advanced parameters:</p> <ul> <li><code>options</code>: additional model parameters listed in the documentation for the Modelfile such as <code>temperature</code></li> <li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li> </ul>"},{"location":"api/#examples_13","title":"Examples","text":""},{"location":"api/#request_34","title":"Request","text":"<pre><code>curl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"all-minilm\",\n  \"prompt\": \"Here is an article about llamas...\"\n}'\n</code></pre>"},{"location":"api/#response_34","title":"Response","text":"<pre><code>{\n  \"embedding\": [\n    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,\n    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281\n  ]\n}\n</code></pre>"},{"location":"api/#version","title":"Version","text":"<pre><code>GET /api/version\n</code></pre> <p>Retrieve the Ollama version</p>"},{"location":"api/#examples_14","title":"Examples","text":""},{"location":"api/#request_35","title":"Request","text":"<pre><code>curl http://localhost:11434/api/version\n</code></pre>"},{"location":"api/#response_35","title":"Response","text":"<pre><code>{\n  \"version\": \"0.5.1\"\n}\n</code></pre>"},{"location":"development/","title":"Development","text":"<p>Install prerequisites:</p> <ul> <li>Go</li> <li>C/C++ Compiler e.g. Clang on macOS, TDM-GCC (Windows amd64) or llvm-mingw (Windows arm64), GCC/Clang on Linux.</li> </ul> <p>Then build and run Ollama from the root directory of the repository:</p> <pre><code>go run . serve\n</code></pre>"},{"location":"development/#macos-apple-silicon","title":"macOS (Apple Silicon)","text":"<p>macOS Apple Silicon supports Metal which is built-in to the Ollama binary. No additional steps are required.</p>"},{"location":"development/#macos-intel","title":"macOS (Intel)","text":"<p>Install prerequisites:</p> <ul> <li>CMake or <code>brew install cmake</code></li> </ul> <p>Then, configure and build the project:</p> <pre><code>cmake -B build\ncmake --build build\n</code></pre> <p>Lastly, run Ollama:</p> <pre><code>go run . serve\n</code></pre>"},{"location":"development/#windows","title":"Windows","text":"<p>Install prerequisites:</p> <ul> <li>CMake</li> <li>Visual Studio 2022 including the Native Desktop Workload</li> <li>(Optional) AMD GPU support<ul> <li>ROCm</li> <li>Ninja</li> </ul> </li> <li>(Optional) NVIDIA GPU support<ul> <li>CUDA SDK</li> </ul> </li> </ul> <p>Then, configure and build the project:</p> <pre><code>cmake -B build\ncmake --build build --config Release\n</code></pre> <p>Tip</p> <p>Building for ROCm requires additional flags: <pre><code>cmake -B build -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\ncmake --build build --config Release\n</code></pre></p> <p>Lastly, run Ollama:</p> <pre><code>go run . serve\n</code></pre>"},{"location":"development/#windows-arm","title":"Windows (ARM)","text":"<p>Windows ARM does not support additional acceleration libraries at this time.  Do not use cmake, simply <code>go run</code> or <code>go build</code>.</p>"},{"location":"development/#linux","title":"Linux","text":"<p>Install prerequisites:</p> <ul> <li>CMake or <code>sudo apt install cmake</code> or <code>sudo dnf install cmake</code></li> <li>(Optional) AMD GPU support<ul> <li>ROCm</li> </ul> </li> <li>(Optional) NVIDIA GPU support<ul> <li>CUDA SDK</li> </ul> </li> </ul> <p>Tip</p> <p>Ensure prerequisites are in <code>PATH</code> before running CMake.</p> <p>Then, configure and build the project:</p> <pre><code>cmake -B build\ncmake --build build\n</code></pre> <p>Lastly, run Ollama:</p> <pre><code>go run . serve\n</code></pre>"},{"location":"development/#docker","title":"Docker","text":"<pre><code>docker build .\n</code></pre>"},{"location":"development/#rocm","title":"ROCm","text":"<pre><code>docker build --build-arg FLAVOR=rocm .\n</code></pre>"},{"location":"development/#running-tests","title":"Running tests","text":"<p>To run tests, use <code>go test</code>:</p> <pre><code>go test ./...\n</code></pre> <p>NOTE: In rare cirumstances, you may need to change a package using the new \"synctest\" package in go1.24.</p> <p>If you do not have the \"synctest\" package enabled, you will not see build or test failures resulting from your change(s), if any, locally, but CI will break.</p> <p>If you see failures in CI, you can either keep pushing changes to see if the CI build passes, or you can enable the \"synctest\" package locally to see the failures before pushing.</p> <p>To enable the \"synctest\" package for testing, run the following command:</p> <pre><code>GOEXPERIMENT=synctest go test ./...\n</code></pre> <p>If you wish to enable synctest for all go commands, you can set the <code>GOEXPERIMENT</code> environment variable in your shell profile or by using:</p> <pre><code>go env -w GOEXPERIMENT=synctest\n</code></pre> <p>Which will enable the \"synctest\" package for all go commands without needing to set it for all shell sessions.</p> <p>The synctest package is not required for production builds.</p>"},{"location":"development/#library-detection","title":"Library detection","text":"<p>Ollama looks for acceleration libraries in the following paths relative to the <code>ollama</code> executable:</p> <ul> <li><code>./lib/ollama</code> (Windows)</li> <li><code>../lib/ollama</code> (Linux)</li> <li><code>.</code> (macOS)</li> <li><code>build/lib/ollama</code> (for development)</li> </ul> <p>If the libraries are not found, Ollama will not run with any acceleration libraries.</p>"},{"location":"docker/","title":"Ollama Docker image","text":""},{"location":"docker/#cpu-only","title":"CPU only","text":"<pre><code>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre>"},{"location":"docker/#nvidia-gpu","title":"Nvidia GPU","text":"<p>Install the NVIDIA Container Toolkit.</p>"},{"location":"docker/#install-with-apt","title":"Install with Apt","text":"<ol> <li> <p>Configure the repository</p> <pre><code>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update\n</code></pre> </li> <li> <p>Install the NVIDIA Container Toolkit packages</p> <pre><code>sudo apt-get install -y nvidia-container-toolkit\n</code></pre> </li> </ol>"},{"location":"docker/#install-with-yum-or-dnf","title":"Install with Yum or Dnf","text":"<ol> <li> <p>Configure the repository</p> <pre><code>curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo \\\n    | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\n</code></pre> </li> <li> <p>Install the NVIDIA Container Toolkit packages</p> <pre><code>sudo yum install -y nvidia-container-toolkit\n</code></pre> </li> </ol>"},{"location":"docker/#configure-docker-to-use-nvidia-driver","title":"Configure Docker to use Nvidia driver","text":"<pre><code>sudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n</code></pre>"},{"location":"docker/#start-the-container","title":"Start the container","text":"<pre><code>docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre> <p>Note</p> <p>If you're running on an NVIDIA JetPack system, Ollama can't automatically discover the correct JetPack version. Pass the environment variable JETSON_JETPACK=5 or JETSON_JETPACK=6 to the container to select version 5 or 6.</p>"},{"location":"docker/#amd-gpu","title":"AMD GPU","text":"<p>To run Ollama using Docker with AMD GPUs, use the <code>rocm</code> tag and the following command:</p> <pre><code>docker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm\n</code></pre>"},{"location":"docker/#run-model-locally","title":"Run model locally","text":"<p>Now you can run a model:</p> <pre><code>docker exec -it ollama ollama run llama3.2\n</code></pre> <p>An interactive session will start when the model starts running. If we are in a CI/CD environment that needs to avoid such session, we can run the model in detached mode:</p> <pre><code>docker exec -d ollama ollama run llama3.2\n</code></pre>"},{"location":"docker/#try-different-models","title":"Try different models","text":"<p>More models can be found on the Ollama library.</p>"},{"location":"examples/","title":"Examples","text":"<p>This directory contains different examples of using Ollama.</p>"},{"location":"examples/#python-examples","title":"Python examples","text":"<p>Ollama Python examples at ollama-python/examples</p>"},{"location":"examples/#javascript-examples","title":"JavaScript examples","text":"<p>Ollama JavaScript examples at ollama-js/examples</p>"},{"location":"examples/#openai-compatibility-examples","title":"OpenAI compatibility examples","text":"<p>Ollama OpenAI compatibility examples at ollama/examples/openai</p>"},{"location":"examples/#community-examples","title":"Community examples","text":"<ul> <li>LangChain Ollama Python</li> <li>LangChain Ollama JS</li> </ul>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#how-can-i-upgrade-ollama","title":"How can I upgrade Ollama?","text":"<p>Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click \"Restart to update\" to apply the update. Updates can also be installed by downloading the latest version manually.</p> <p>On Linux, re-run the install script:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"faq/#how-can-i-view-the-logs","title":"How can I view the logs?","text":"<p>Review the Troubleshooting docs for more about using logs.</p>"},{"location":"faq/#is-my-gpu-compatible-with-ollama","title":"Is my GPU compatible with Ollama?","text":"<p>Please refer to the GPU docs.</p>"},{"location":"faq/#how-can-i-specify-the-context-window-size","title":"How can I specify the context window size?","text":"<p>By default, Ollama uses a context window size of 4096 tokens. </p> <p>This can be overridden with the <code>OLLAMA_CONTEXT_LENGTH</code> environment variable. For example, to set the default context window to 8K, use: </p> <pre><code>OLLAMA_CONTEXT_LENGTH=8192 ollama serve\n</code></pre> <p>To change this when using <code>ollama run</code>, use <code>/set parameter</code>:</p> <pre><code>/set parameter num_ctx 4096\n</code></pre> <p>When using the API, specify the <code>num_ctx</code> parameter:</p> <pre><code>curl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 4096\n  }\n}'\n</code></pre>"},{"location":"faq/#how-can-i-tell-if-my-model-was-loaded-onto-the-gpu","title":"How can I tell if my model was loaded onto the GPU?","text":"<p>Use the <code>ollama ps</code> command to see what models are currently loaded into memory.</p> <pre><code>ollama ps\n</code></pre> <p>Output:</p> <pre><code>NAME          ID              SIZE    PROCESSOR   UNTIL\nllama3:70b    bcfb190ca3a7    42 GB   100% GPU    4 minutes from now\n</code></pre> <p>The <code>Processor</code> column will show which memory the model was loaded in to: * <code>100% GPU</code> means the model was loaded entirely into the GPU * <code>100% CPU</code> means the model was loaded entirely in system memory * <code>48%/52% CPU/GPU</code> means the model was loaded partially onto both the GPU and into system memory</p>"},{"location":"faq/#how-do-i-configure-ollama-server","title":"How do I configure Ollama server?","text":"<p>Ollama server can be configured with environment variables.</p>"},{"location":"faq/#setting-environment-variables-on-mac","title":"Setting environment variables on Mac","text":"<p>If Ollama is run as a macOS application, environment variables should be set using <code>launchctl</code>:</p> <ol> <li> <p>For each environment variable, call <code>launchctl setenv</code>.</p> <pre><code>launchctl setenv OLLAMA_HOST \"0.0.0.0:11434\"\n</code></pre> </li> <li> <p>Restart Ollama application.</p> </li> </ol>"},{"location":"faq/#setting-environment-variables-on-linux","title":"Setting environment variables on Linux","text":"<p>If Ollama is run as a systemd service, environment variables should be set using <code>systemctl</code>:</p> <ol> <li> <p>Edit the systemd service by calling <code>systemctl edit ollama.service</code>. This will open an editor.</p> </li> <li> <p>For each environment variable, add a line <code>Environment</code> under section <code>[Service]</code>:</p> <pre><code>[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\n</code></pre> </li> <li> <p>Save and exit.</p> </li> <li> <p>Reload <code>systemd</code> and restart Ollama:</p> </li> </ol> <pre><code>systemctl daemon-reload\nsystemctl restart ollama\n</code></pre>"},{"location":"faq/#setting-environment-variables-on-windows","title":"Setting environment variables on Windows","text":"<p>On Windows, Ollama inherits your user and system environment variables.</p> <ol> <li> <p>First Quit Ollama by clicking on it in the task bar.</p> </li> <li> <p>Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for environment variables.</p> </li> <li> <p>Click on Edit environment variables for your account.</p> </li> <li> <p>Edit or create a new variable for your user account for <code>OLLAMA_HOST</code>, <code>OLLAMA_MODELS</code>, etc.</p> </li> <li> <p>Click OK/Apply to save.</p> </li> <li> <p>Start the Ollama application from the Windows Start menu.</p> </li> </ol>"},{"location":"faq/#how-do-i-use-ollama-behind-a-proxy","title":"How do I use Ollama behind a proxy?","text":"<p>Ollama pulls models from the Internet and may require a proxy server to access the models. Use <code>HTTPS_PROXY</code> to redirect outbound requests through the proxy. Ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.</p> <p>Note</p> <p>Avoid setting <code>HTTP_PROXY</code>. Ollama does not use HTTP for model pulls, only HTTPS. Setting <code>HTTP_PROXY</code> may interrupt client connections to the server.</p>"},{"location":"faq/#how-do-i-use-ollama-behind-a-proxy-in-docker","title":"How do I use Ollama behind a proxy in Docker?","text":"<p>The Ollama Docker container image can be configured to use a proxy by passing <code>-e HTTPS_PROXY=https://proxy.example.com</code> when starting the container.</p> <p>Alternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on macOS, Windows, and Linux, and Docker daemon with systemd.</p> <p>Ensure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.</p> <pre><code>FROM ollama/ollama\nCOPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt\nRUN update-ca-certificates\n</code></pre> <p>Build and run this image:</p> <pre><code>docker build -t ollama-with-ca .\ndocker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca\n</code></pre>"},{"location":"faq/#does-ollama-send-my-prompts-and-answers-back-to-ollamacom","title":"Does Ollama send my prompts and answers back to ollama.com?","text":"<p>No. Ollama runs locally, and conversation data does not leave your machine.</p>"},{"location":"faq/#how-can-i-expose-ollama-on-my-network","title":"How can I expose Ollama on my network?","text":"<p>Ollama binds 127.0.0.1 port 11434 by default. Change the bind address with the <code>OLLAMA_HOST</code> environment variable.</p> <p>Refer to the section above for how to set environment variables on your platform.</p>"},{"location":"faq/#how-can-i-use-ollama-with-a-proxy-server","title":"How can I use Ollama with a proxy server?","text":"<p>Ollama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:</p> <pre><code>server {\n    listen 80;\n    server_name example.com;  # Replace with your domain or IP\n    location / {\n        proxy_pass http://localhost:11434;\n        proxy_set_header Host localhost:11434;\n    }\n}\n</code></pre>"},{"location":"faq/#how-can-i-use-ollama-with-ngrok","title":"How can I use Ollama with ngrok?","text":"<p>Ollama can be accessed using a range of tools for tunneling tools. For example with Ngrok:</p> <pre><code>ngrok http 11434 --host-header=\"localhost:11434\"\n</code></pre>"},{"location":"faq/#how-can-i-use-ollama-with-cloudflare-tunnel","title":"How can I use Ollama with Cloudflare Tunnel?","text":"<p>To use Ollama with Cloudflare Tunnel, use the <code>--url</code> and <code>--http-host-header</code> flags:</p> <pre><code>cloudflared tunnel --url http://localhost:11434 --http-host-header=\"localhost:11434\"\n</code></pre>"},{"location":"faq/#how-can-i-allow-additional-web-origins-to-access-ollama","title":"How can I allow additional web origins to access Ollama?","text":"<p>Ollama allows cross-origin requests from <code>127.0.0.1</code> and <code>0.0.0.0</code> by default. Additional origins can be configured with <code>OLLAMA_ORIGINS</code>.</p> <p>For browser extensions, you'll need to explicitly allow the extension's origin pattern. Set <code>OLLAMA_ORIGINS</code> to include <code>chrome-extension://*</code>, <code>moz-extension://*</code>, and <code>safari-web-extension://*</code> if you wish to allow all browser extensions access, or specific extensions as needed:</p> <pre><code># Allow all Chrome, Firefox, and Safari extensions\nOLLAMA_ORIGINS=chrome-extension://*,moz-extension://*,safari-web-extension://* ollama serve\n</code></pre> <p>Refer to the section above for how to set environment variables on your platform.</p>"},{"location":"faq/#where-are-models-stored","title":"Where are models stored?","text":"<ul> <li>macOS: <code>~/.ollama/models</code></li> <li>Linux: <code>/usr/share/ollama/.ollama/models</code></li> <li>Windows: <code>C:\\Users\\%username%\\.ollama\\models</code></li> </ul>"},{"location":"faq/#how-do-i-set-them-to-a-different-location","title":"How do I set them to a different location?","text":"<p>If a different directory needs to be used, set the environment variable <code>OLLAMA_MODELS</code> to the chosen directory.</p> <p>Note: on Linux using the standard installer, the <code>ollama</code> user needs read and write access to the specified directory. To assign the directory to the <code>ollama</code> user run <code>sudo chown -R ollama:ollama &lt;directory&gt;</code>.</p> <p>Refer to the section above for how to set environment variables on your platform.</p>"},{"location":"faq/#how-can-i-use-ollama-in-visual-studio-code","title":"How can I use Ollama in Visual Studio Code?","text":"<p>There is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of extensions &amp; plugins at the bottom of the main repository readme.</p>"},{"location":"faq/#how-do-i-use-ollama-with-gpu-acceleration-in-docker","title":"How do I use Ollama with GPU acceleration in Docker?","text":"<p>The Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the nvidia-container-toolkit. See ollama/ollama for more details.</p> <p>GPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.</p>"},{"location":"faq/#why-is-networking-slow-in-wsl2-on-windows-10","title":"Why is networking slow in WSL2 on Windows 10?","text":"<p>This can impact both installing Ollama, as well as downloading models.</p> <p>Open <code>Control Panel &gt; Networking and Internet &gt; View network status and tasks</code> and click on <code>Change adapter settings</code> on the left panel. Find the <code>vEthernel (WSL)</code> adapter, right click and select <code>Properties</code>. Click on <code>Configure</code> and open the <code>Advanced</code> tab. Search through each of the properties until you find <code>Large Send Offload Version 2 (IPv4)</code> and <code>Large Send Offload Version 2 (IPv6)</code>. Disable both of these properties.</p>"},{"location":"faq/#how-can-i-preload-a-model-into-ollama-to-get-faster-response-times","title":"How can I preload a model into Ollama to get faster response times?","text":"<p>If you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the <code>/api/generate</code> and <code>/api/chat</code> API endpoints.</p> <p>To preload the mistral model using the generate endpoint, use:</p> <pre><code>curl http://localhost:11434/api/generate -d '{\"model\": \"mistral\"}'\n</code></pre> <p>To use the chat completions endpoint, use:</p> <pre><code>curl http://localhost:11434/api/chat -d '{\"model\": \"mistral\"}'\n</code></pre> <p>To preload a model using the CLI, use the command:</p> <pre><code>ollama run llama3.2 \"\"\n</code></pre>"},{"location":"faq/#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately","title":"How do I keep a model loaded in memory or make it unload immediately?","text":"<p>By default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you're making numerous requests to the LLM. If you want to immediately unload a model from memory, use the <code>ollama stop</code> command:</p> <pre><code>ollama stop llama3.2\n</code></pre> <p>If you're using the API, use the <code>keep_alive</code> parameter with the <code>/api/generate</code> and <code>/api/chat</code> endpoints to set the amount of time that a model stays in memory. The <code>keep_alive</code> parameter can be set to: * a duration string (such as \"10m\" or \"24h\") * a number in seconds (such as 3600) * any negative number which will keep the model loaded in memory (e.g. -1 or \"-1m\") * '0' which will unload the model immediately after generating a response</p> <p>For example, to preload a model and leave it in memory use:</p> <pre><code>curl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": -1}'\n</code></pre> <p>To unload the model and free up memory use:</p> <pre><code>curl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": 0}'\n</code></pre> <p>Alternatively, you can change the amount of time all models are loaded into memory by setting the <code>OLLAMA_KEEP_ALIVE</code> environment variable when starting the Ollama server. The <code>OLLAMA_KEEP_ALIVE</code> variable uses the same parameter types as the <code>keep_alive</code> parameter types mentioned above. Refer to the section explaining how to configure the Ollama server to correctly set the environment variable.</p> <p>The <code>keep_alive</code> API parameter with the <code>/api/generate</code> and <code>/api/chat</code> API endpoints will override the <code>OLLAMA_KEEP_ALIVE</code> setting.</p>"},{"location":"faq/#how-do-i-manage-the-maximum-number-of-requests-the-ollama-server-can-queue","title":"How do I manage the maximum number of requests the Ollama server can queue?","text":"<p>If too many requests are sent to the server, it will respond with a 503 error indicating the server is overloaded.  You can adjust how many requests may be queue by setting <code>OLLAMA_MAX_QUEUE</code>.</p>"},{"location":"faq/#how-does-ollama-handle-concurrent-requests","title":"How does Ollama handle concurrent requests?","text":"<p>Ollama supports two levels of concurrent processing.  If your system has sufficient available memory (system memory when using CPU inference, or VRAM for GPU inference) then multiple models can be loaded at the same time.  For a given model, if there is sufficient available memory when the model is loaded, it is configured to allow parallel request processing.</p> <p>If there is insufficient available memory to load a new model request while one or more models are already loaded, all new requests will be queued until the new model can be loaded.  As prior models become idle, one or more will be unloaded to make room for the new model.  Queued requests will be processed in order.  When using GPU inference new models must be able to completely fit in VRAM to allow concurrent model loads.</p> <p>Parallel request processing for a given model results in increasing the context size by the number of parallel requests.  For example, a 2K context with 4 parallel requests will result in an 8K context and additional memory allocation.</p> <p>The following server settings may be used to adjust how Ollama handles concurrent requests on most platforms:</p> <ul> <li><code>OLLAMA_MAX_LOADED_MODELS</code> - The maximum number of models that can be loaded concurrently provided they fit in available memory.  The default is 3 * the number of GPUs or 3 for CPU inference.</li> <li><code>OLLAMA_NUM_PARALLEL</code> - The maximum number of parallel requests each model will process at the same time.  The default will auto-select either 4 or 1 based on available memory.</li> <li><code>OLLAMA_MAX_QUEUE</code> - The maximum number of requests Ollama will queue when busy before rejecting additional requests. The default is 512</li> </ul> <p>Note: Windows with Radeon GPUs currently default to 1 model maximum due to limitations in ROCm v5.7 for available VRAM reporting.  Once ROCm v6.2 is available, Windows Radeon will follow the defaults above.  You may enable concurrent model loads on Radeon on Windows, but ensure you don't load more models than will fit into your GPUs VRAM.</p>"},{"location":"faq/#how-does-ollama-load-models-on-multiple-gpus","title":"How does Ollama load models on multiple GPUs?","text":"<p>When loading a new model, Ollama evaluates the required VRAM for the model against what is currently available.  If the model will entirely fit on any single GPU, Ollama will load the model on that GPU.  This typically provides the best performance as it reduces the amount of data transferring across the PCI bus during inference.  If the model does not fit entirely on one GPU, then it will be spread across all the available GPUs.</p>"},{"location":"faq/#how-can-i-enable-flash-attention","title":"How can I enable Flash Attention?","text":"<p>Flash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows.  To enable Flash Attention, set the <code>OLLAMA_FLASH_ATTENTION</code> environment variable to <code>1</code> when starting the Ollama server.</p>"},{"location":"faq/#how-can-i-set-the-quantization-type-for-the-kv-cache","title":"How can I set the quantization type for the K/V cache?","text":"<p>The K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.</p> <p>To use quantized K/V cache with Ollama you can set the following environment variable:</p> <ul> <li><code>OLLAMA_KV_CACHE_TYPE</code> - The quantization type for the K/V cache.  Default is <code>f16</code>.</li> </ul> <p>Note: Currently this is a global option - meaning all models will run with the specified quantization type.</p> <p>The currently available K/V cache quantization types are:</p> <ul> <li><code>f16</code> - high precision and memory usage (default).</li> <li><code>q8_0</code> - 8-bit quantization, uses approximately 1/2 the memory of <code>f16</code> with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).</li> <li><code>q4_0</code> - 4-bit quantization, uses approximately 1/4 the memory of <code>f16</code> with a small-medium loss in precision that may be more noticeable at higher context sizes.</li> </ul> <p>How much the cache quantization impacts the model's response quality will depend on the model and the task.  Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.</p> <p>You may need to experiment with different quantization types to find the best balance between memory usage and quality.</p>"},{"location":"gpu/","title":"GPU","text":""},{"location":"gpu/#nvidia","title":"Nvidia","text":"<p>Ollama supports Nvidia GPUs with compute capability 5.0+ and driver version 531 and newer.</p> <p>Check your compute compatibility to see if your card is supported: https://developer.nvidia.com/cuda-gpus</p> Compute Capability Family Cards 9.0 NVIDIA <code>H200</code> <code>H100</code> 8.9 GeForce RTX 40xx <code>RTX 4090</code> <code>RTX 4080 SUPER</code> <code>RTX 4080</code> <code>RTX 4070 Ti SUPER</code> <code>RTX 4070 Ti</code> <code>RTX 4070 SUPER</code> <code>RTX 4070</code> <code>RTX 4060 Ti</code> <code>RTX 4060</code> NVIDIA Professional <code>L4</code> <code>L40</code> <code>RTX 6000</code> 8.6 GeForce RTX 30xx <code>RTX 3090 Ti</code> <code>RTX 3090</code> <code>RTX 3080 Ti</code> <code>RTX 3080</code> <code>RTX 3070 Ti</code> <code>RTX 3070</code> <code>RTX 3060 Ti</code> <code>RTX 3060</code> <code>RTX 3050 Ti</code> <code>RTX 3050</code> NVIDIA Professional <code>A40</code> <code>RTX A6000</code> <code>RTX A5000</code> <code>RTX A4000</code> <code>RTX A3000</code> <code>RTX A2000</code> <code>A10</code> <code>A16</code> <code>A2</code> 8.0 NVIDIA <code>A100</code> <code>A30</code> 7.5 GeForce GTX/RTX <code>GTX 1650 Ti</code> <code>TITAN RTX</code> <code>RTX 2080 Ti</code> <code>RTX 2080</code> <code>RTX 2070</code> <code>RTX 2060</code> NVIDIA Professional <code>T4</code> <code>RTX 5000</code> <code>RTX 4000</code> <code>RTX 3000</code> <code>T2000</code> <code>T1200</code> <code>T1000</code> <code>T600</code> <code>T500</code> Quadro <code>RTX 8000</code> <code>RTX 6000</code> <code>RTX 5000</code> <code>RTX 4000</code> 7.0 NVIDIA <code>TITAN V</code> <code>V100</code> <code>Quadro GV100</code> 6.1 NVIDIA TITAN <code>TITAN Xp</code> <code>TITAN X</code> GeForce GTX <code>GTX 1080 Ti</code> <code>GTX 1080</code> <code>GTX 1070 Ti</code> <code>GTX 1070</code> <code>GTX 1060</code> <code>GTX 1050 Ti</code> <code>GTX 1050</code> Quadro <code>P6000</code> <code>P5200</code> <code>P4200</code> <code>P3200</code> <code>P5000</code> <code>P4000</code> <code>P3000</code> <code>P2200</code> <code>P2000</code> <code>P1000</code> <code>P620</code> <code>P600</code> <code>P500</code> <code>P520</code> Tesla <code>P40</code> <code>P4</code> 6.0 NVIDIA <code>Tesla P100</code> <code>Quadro GP100</code> 5.2 GeForce GTX <code>GTX TITAN X</code> <code>GTX 980 Ti</code> <code>GTX 980</code> <code>GTX 970</code> <code>GTX 960</code> <code>GTX 950</code> Quadro <code>M6000 24GB</code> <code>M6000</code> <code>M5000</code> <code>M5500M</code> <code>M4000</code> <code>M2200</code> <code>M2000</code> <code>M620</code> Tesla <code>M60</code> <code>M40</code> 5.0 GeForce GTX <code>GTX 750 Ti</code> <code>GTX 750</code> <code>NVS 810</code> Quadro <code>K2200</code> <code>K1200</code> <code>K620</code> <code>M1200</code> <code>M520</code> <code>M5000M</code> <code>M4000M</code> <code>M3000M</code> <code>M2000M</code> <code>M1000M</code> <code>K620M</code> <code>M600M</code> <code>M500M</code> <p>For building locally to support older GPUs, see developer.md</p>"},{"location":"gpu/#gpu-selection","title":"GPU Selection","text":"<p>If you have multiple NVIDIA GPUs in your system and want to limit Ollama to use a subset, you can set <code>CUDA_VISIBLE_DEVICES</code> to a comma separated list of GPUs. Numeric IDs may be used, however ordering may vary, so UUIDs are more reliable. You can discover the UUID of your GPUs by running <code>nvidia-smi -L</code> If you want to ignore the GPUs and force CPU usage, use an invalid GPU ID (e.g., \"-1\")</p>"},{"location":"gpu/#linux-suspend-resume","title":"Linux Suspend Resume","text":"<p>On linux, after a suspend/resume cycle, sometimes Ollama will fail to discover your NVIDIA GPU, and fallback to running on the CPU.  You can workaround this driver bug by reloading the NVIDIA UVM driver with <code>sudo rmmod nvidia_uvm &amp;&amp; sudo modprobe nvidia_uvm</code></p>"},{"location":"gpu/#amd-radeon","title":"AMD Radeon","text":"<p>Ollama supports the following AMD GPUs:</p>"},{"location":"gpu/#linux-support","title":"Linux Support","text":"Family Cards and accelerators AMD Radeon RX <code>7900 XTX</code> <code>7900 XT</code> <code>7900 GRE</code> <code>7800 XT</code> <code>7700 XT</code> <code>7600 XT</code> <code>7600</code> <code>6950 XT</code> <code>6900 XTX</code> <code>6900XT</code> <code>6800 XT</code> <code>6800</code> <code>Vega 64</code> <code>Vega 56</code> AMD Radeon PRO <code>W7900</code> <code>W7800</code> <code>W7700</code> <code>W7600</code> <code>W7500</code> <code>W6900X</code> <code>W6800X Duo</code> <code>W6800X</code> <code>W6800</code> <code>V620</code> <code>V420</code> <code>V340</code> <code>V320</code> <code>Vega II Duo</code> <code>Vega II</code> <code>VII</code> <code>SSG</code> AMD Instinct <code>MI300X</code> <code>MI300A</code> <code>MI300</code> <code>MI250X</code> <code>MI250</code> <code>MI210</code> <code>MI200</code> <code>MI100</code> <code>MI60</code> <code>MI50</code>"},{"location":"gpu/#windows-support","title":"Windows Support","text":"<p>With ROCm v6.1, the following GPUs are supported on Windows.</p> Family Cards and accelerators AMD Radeon RX <code>7900 XTX</code> <code>7900 XT</code> <code>7900 GRE</code> <code>7800 XT</code> <code>7700 XT</code> <code>7600 XT</code> <code>7600</code> <code>6950 XT</code> <code>6900 XTX</code> <code>6900XT</code> <code>6800 XT</code> <code>6800</code> AMD Radeon PRO <code>W7900</code> <code>W7800</code> <code>W7700</code> <code>W7600</code> <code>W7500</code> <code>W6900X</code> <code>W6800X Duo</code> <code>W6800X</code> <code>W6800</code> <code>V620</code>"},{"location":"gpu/#overrides-on-linux","title":"Overrides on Linux","text":"<p>Ollama leverages the AMD ROCm library, which does not support all AMD GPUs. In some cases you can force the system to try to use a similar LLVM target that is close.  For example The Radeon RX 5400 is <code>gfx1034</code> (also known as 10.3.4) however, ROCm does not currently support this target. The closest support is <code>gfx1030</code>.  You can use the environment variable <code>HSA_OVERRIDE_GFX_VERSION</code> with <code>x.y.z</code> syntax.  So for example, to force the system to run on the RX 5400, you would set <code>HSA_OVERRIDE_GFX_VERSION=\"10.3.0\"</code> as an environment variable for the server.  If you have an unsupported AMD GPU you can experiment using the list of supported types below.</p> <p>If you have multiple GPUs with different GFX versions, append the numeric device number to the environment variable to set them individually.  For example, <code>HSA_OVERRIDE_GFX_VERSION_0=10.3.0</code> and  <code>HSA_OVERRIDE_GFX_VERSION_1=11.0.0</code></p> <p>At this time, the known supported GPU types on linux are the following LLVM Targets. This table shows some example GPUs that map to these LLVM targets: | LLVM Target | An Example GPU | |-----------------|---------------------| | gfx900 | Radeon RX Vega 56 | | gfx906 | Radeon Instinct MI50 | | gfx908 | Radeon Instinct MI100 | | gfx90a | Radeon Instinct MI210 | | gfx940 | Radeon Instinct MI300 | | gfx941 | | | gfx942 | | | gfx1030 | Radeon PRO V620 | | gfx1100 | Radeon PRO W7900 | | gfx1101 | Radeon PRO W7700 | | gfx1102 | Radeon RX 7600 |</p> <p>AMD is working on enhancing ROCm v6 to broaden support for families of GPUs in a future release which should increase support for more GPUs.</p> <p>Reach out on Discord or file an issue for additional help.</p>"},{"location":"gpu/#gpu-selection_1","title":"GPU Selection","text":"<p>If you have multiple AMD GPUs in your system and want to limit Ollama to use a subset, you can set <code>ROCR_VISIBLE_DEVICES</code> to a comma separated list of GPUs. You can see the list of devices with <code>rocminfo</code>.  If you want to ignore the GPUs and force CPU usage, use an invalid GPU ID (e.g., \"-1\").  When available, use the <code>Uuid</code> to uniquely identify the device instead of numeric value.</p>"},{"location":"gpu/#container-permission","title":"Container Permission","text":"<p>In some Linux distributions, SELinux can prevent containers from accessing the AMD GPU devices.  On the host system you can run  <code>sudo setsebool container_use_devices=1</code> to allow containers to use devices.</p>"},{"location":"gpu/#metal-apple-gpus","title":"Metal (Apple GPUs)","text":"<p>Ollama supports GPU acceleration on Apple devices via the Metal API.</p>"},{"location":"import/","title":"Importing a model","text":""},{"location":"import/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Importing a Safetensors adapter</li> <li>Importing a Safetensors model</li> <li>Importing a GGUF file</li> <li>Sharing models on ollama.com</li> </ul>"},{"location":"import/#importing-a-fine-tuned-adapter-from-safetensors-weights","title":"Importing a fine tuned adapter from Safetensors weights","text":"<p>First, create a <code>Modelfile</code> with a <code>FROM</code> command pointing at the base model you used for fine tuning, and an <code>ADAPTER</code> command which points to the directory with your Safetensors adapter:</p> <pre><code>FROM &lt;base model name&gt;\nADAPTER /path/to/safetensors/adapter/directory\n</code></pre> <p>Make sure that you use the same base model in the <code>FROM</code> command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your <code>Modelfile</code>, use <code>ADAPTER .</code> to specify the adapter path.</p> <p>Now run <code>ollama create</code> from the directory where the <code>Modelfile</code> was created:</p> <pre><code>ollama create my-model\n</code></pre> <p>Lastly, test the model:</p> <pre><code>ollama run my-model\n</code></pre> <p>Ollama supports importing adapters based on several different model architectures including:</p> <ul> <li>Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);</li> <li>Mistral (including Mistral 1, Mistral 2, and Mixtral); and</li> <li>Gemma (including Gemma 1 and Gemma 2)</li> </ul> <p>You can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:</p> <ul> <li>Hugging Face fine tuning framework</li> <li>Unsloth</li> <li>MLX</li> </ul>"},{"location":"import/#importing-a-model-from-safetensors-weights","title":"Importing a model from Safetensors weights","text":"<p>First, create a <code>Modelfile</code> with a <code>FROM</code> command which points to the directory containing your Safetensors weights:</p> <pre><code>FROM /path/to/safetensors/directory\n</code></pre> <p>If you create the Modelfile in the same directory as the weights, you can use the command <code>FROM .</code>.</p> <p>Now run the <code>ollama create</code> command from the directory where you created the <code>Modelfile</code>:</p> <pre><code>ollama create my-model\n</code></pre> <p>Lastly, test the model:</p> <pre><code>ollama run my-model\n</code></pre> <p>Ollama supports importing models for several different architectures including:</p> <ul> <li>Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);</li> <li>Mistral (including Mistral 1, Mistral 2, and Mixtral);</li> <li>Gemma (including Gemma 1 and Gemma 2); and</li> <li>Phi3</li> </ul> <p>This includes importing foundation models as well as any fine tuned models which have been fused with a foundation model.</p>"},{"location":"import/#importing-a-gguf-based-model-or-adapter","title":"Importing a GGUF based model or adapter","text":"<p>If you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:</p> <ul> <li>converting a Safetensors model with the <code>convert_hf_to_gguf.py</code> from Llama.cpp; </li> <li>converting a Safetensors adapter with the <code>convert_lora_to_gguf.py</code> from Llama.cpp; or</li> <li>downloading a model or adapter from a place such as HuggingFace</li> </ul> <p>To import a GGUF model, create a <code>Modelfile</code> containing:</p> <pre><code>FROM /path/to/file.gguf\n</code></pre> <p>For a GGUF adapter, create the <code>Modelfile</code> with:</p> <pre><code>FROM &lt;model name&gt;\nADAPTER /path/to/file.gguf\n</code></pre> <p>When importing a GGUF adapter, it's important to use the same base model as the base model that the adapter was created with. You can use:</p> <ul> <li>a model from Ollama</li> <li>a GGUF file</li> <li>a Safetensors based model </li> </ul> <p>Once you have created your <code>Modelfile</code>, use the <code>ollama create</code> command to build the model.</p> <pre><code>ollama create my-model\n</code></pre>"},{"location":"import/#quantizing-a-model","title":"Quantizing a Model","text":"<p>Quantizing a model allows you to run models faster and with less memory consumption but at reduced accuracy. This allows you to run a model on more modest hardware.</p> <p>Ollama can quantize FP16 and FP32 based models into different quantization levels using the <code>-q/--quantize</code> flag with the <code>ollama create</code> command.</p> <p>First, create a Modelfile with the FP16 or FP32 based model you wish to quantize.</p> <pre><code>FROM /path/to/my/gemma/f16/model\n</code></pre> <p>Use <code>ollama create</code> to then create the quantized model.</p> <pre><code>$ ollama create --quantize q4_K_M mymodel\ntransferring model data\nquantizing F16 model to Q4_K_M\ncreating new layer sha256:735e246cc1abfd06e9cdcf95504d6789a6cd1ad7577108a70d9902fef503c1bd\ncreating new layer sha256:0853f0ad24e5865173bbf9ffcc7b0f5d56b66fd690ab1009867e45e7d2c4db0f\nwriting manifest\nsuccess\n</code></pre>"},{"location":"import/#supported-quantizations","title":"Supported Quantizations","text":"<ul> <li><code>q8_0</code></li> </ul>"},{"location":"import/#k-means-quantizations","title":"K-means Quantizations","text":"<ul> <li><code>q4_K_S</code></li> <li><code>q4_K_M</code></li> </ul>"},{"location":"import/#sharing-your-model-on-ollamacom","title":"Sharing your model on ollama.com","text":"<p>You can share any model you have created by pushing it to ollama.com so that other users can try it out.</p> <p>First, use your browser to go to the Ollama Sign-Up page. If you already have an account, you can skip this step.</p> <p></p> <p>The <code>Username</code> field will be used as part of your model's name (e.g. <code>jmorganca/mymodel</code>), so make sure you are comfortable with the username that you have selected.</p> <p>Now that you have created an account and are signed-in, go to the Ollama Keys Settings page.</p> <p>Follow the directions on the page to determine where your Ollama Public Key is located.</p> <p></p> <p>Click on the <code>Add Ollama Public Key</code> button, and copy and paste the contents of your Ollama Public Key into the text field.</p> <p>To push a model to ollama.com, first make sure that it is named correctly with your username. You may have to use the <code>ollama cp</code> command to copy your model to give it the correct name. Once you're happy with your model's name, use the <code>ollama push</code> command to push it to ollama.com.</p> <pre><code>ollama cp mymodel myuser/mymodel\nollama push myuser/mymodel\n</code></pre> <p>Once your model has been pushed, other users can pull and run it by using the command:</p> <pre><code>ollama run myuser/mymodel\n</code></pre>"},{"location":"linux/","title":"Linux","text":""},{"location":"linux/#install","title":"Install","text":"<p>To install Ollama, run the following command:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"linux/#manual-install","title":"Manual install","text":"<p>Note</p> <p>If you are upgrading from a prior version, you should remove the old libraries with <code>sudo rm -rf /usr/lib/ollama</code> first.</p> <p>Download and extract the package:</p> <pre><code>curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n</code></pre> <p>Start Ollama:</p> <pre><code>ollama serve\n</code></pre> <p>In another terminal, verify that Ollama is running:</p> <pre><code>ollama -v\n</code></pre>"},{"location":"linux/#amd-gpu-install","title":"AMD GPU install","text":"<p>If you have an AMD GPU, also download and extract the additional ROCm package:</p> <pre><code>curl -L https://ollama.com/download/ollama-linux-amd64-rocm.tgz -o ollama-linux-amd64-rocm.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64-rocm.tgz\n</code></pre>"},{"location":"linux/#arm64-install","title":"ARM64 install","text":"<p>Download and extract the ARM64-specific package:</p> <pre><code>curl -L https://ollama.com/download/ollama-linux-arm64.tgz -o ollama-linux-arm64.tgz\nsudo tar -C /usr -xzf ollama-linux-arm64.tgz\n</code></pre>"},{"location":"linux/#adding-ollama-as-a-startup-service-recommended","title":"Adding Ollama as a startup service (recommended)","text":"<p>Create a user and group for Ollama:</p> <pre><code>sudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama\nsudo usermod -a -G ollama $(whoami)\n</code></pre> <p>Create a service file in <code>/etc/systemd/system/ollama.service</code>:</p> <pre><code>[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=$PATH\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then start the service:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable ollama\n</code></pre>"},{"location":"linux/#install-cuda-drivers-optional","title":"Install CUDA drivers (optional)","text":"<p>Download and install CUDA.</p> <p>Verify that the drivers are installed by running the following command, which should print details about your GPU:</p> <pre><code>nvidia-smi\n</code></pre>"},{"location":"linux/#install-amd-rocm-drivers-optional","title":"Install AMD ROCm drivers (optional)","text":"<p>Download and Install ROCm v6.</p>"},{"location":"linux/#start-ollama","title":"Start Ollama","text":"<p>Start Ollama and verify it is running:</p> <pre><code>sudo systemctl start ollama\nsudo systemctl status ollama\n</code></pre> <p>Note</p> <p>While AMD has contributed the <code>amdgpu</code> driver upstream to the official linux kernel source, the version is older and may not support all ROCm features. We recommend you install the latest driver from AMD for best support of your Radeon GPU.</p>"},{"location":"linux/#customizing","title":"Customizing","text":"<p>To customize the installation of Ollama, you can edit the systemd service file or the environment variables by running:</p> <pre><code>sudo systemctl edit ollama\n</code></pre> <p>Alternatively, create an override file manually in <code>/etc/systemd/system/ollama.service.d/override.conf</code>:</p> <pre><code>[Service]\nEnvironment=\"OLLAMA_DEBUG=1\"\n</code></pre>"},{"location":"linux/#updating","title":"Updating","text":"<p>Update Ollama by running the install script again:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p>Or by re-downloading Ollama:</p> <pre><code>curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n</code></pre>"},{"location":"linux/#installing-specific-versions","title":"Installing specific versions","text":"<p>Use <code>OLLAMA_VERSION</code> environment variable with the install script to install a specific version of Ollama, including pre-releases. You can find the version numbers in the releases page.</p> <p>For example:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh\n</code></pre>"},{"location":"linux/#viewing-logs","title":"Viewing logs","text":"<p>To view logs of Ollama running as a startup service, run:</p> <pre><code>journalctl -e -u ollama\n</code></pre>"},{"location":"linux/#uninstall","title":"Uninstall","text":"<p>Remove the ollama service:</p> <pre><code>sudo systemctl stop ollama\nsudo systemctl disable ollama\nsudo rm /etc/systemd/system/ollama.service\n</code></pre> <p>Remove the ollama binary from your bin directory (either <code>/usr/local/bin</code>, <code>/usr/bin</code>, or <code>/bin</code>):</p> <pre><code>sudo rm $(which ollama)\n</code></pre> <p>Remove the downloaded models and Ollama service user and group:</p> <pre><code>sudo rm -r /usr/share/ollama\nsudo userdel ollama\nsudo groupdel ollama\n</code></pre> <p>Remove installed libraries:</p> <pre><code>sudo rm -rf /usr/local/lib/ollama\n</code></pre>"},{"location":"modelfile/","title":"Ollama Model File","text":"<p>Note</p> <p><code>Modelfile</code> syntax is in development</p> <p>A model file is the blueprint to create and share models with Ollama.</p>"},{"location":"modelfile/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Format</li> <li>Examples</li> <li>Instructions</li> <li>FROM (Required)<ul> <li>Build from existing model</li> <li>Build from a Safetensors model</li> <li>Build from a GGUF file</li> </ul> </li> <li>PARAMETER<ul> <li>Valid Parameters and Values</li> </ul> </li> <li>TEMPLATE<ul> <li>Template Variables</li> </ul> </li> <li>SYSTEM</li> <li>ADAPTER</li> <li>LICENSE</li> <li>MESSAGE</li> <li>Notes</li> </ul>"},{"location":"modelfile/#format","title":"Format","text":"<p>The format of the <code>Modelfile</code>:</p> <pre><code># comment\nINSTRUCTION arguments\n</code></pre> Instruction Description <code>FROM</code> (required) Defines the base model to use. <code>PARAMETER</code> Sets the parameters for how Ollama will run the model. <code>TEMPLATE</code> The full prompt template to be sent to the model. <code>SYSTEM</code> Specifies the system message that will be set in the template. <code>ADAPTER</code> Defines the (Q)LoRA adapters to apply to the model. <code>LICENSE</code> Specifies the legal license. <code>MESSAGE</code> Specify message history."},{"location":"modelfile/#examples","title":"Examples","text":""},{"location":"modelfile/#basic-modelfile","title":"Basic <code>Modelfile</code>","text":"<p>An example of a <code>Modelfile</code> creating a mario blueprint:</p> <pre><code>FROM llama3.2\n# sets the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token\nPARAMETER num_ctx 4096\n\n# sets a custom system message to specify the behavior of the chat assistant\nSYSTEM You are Mario from super mario bros, acting as an assistant.\n</code></pre> <p>To use this:</p> <ol> <li>Save it as a file (e.g. <code>Modelfile</code>)</li> <li><code>ollama create choose-a-model-name -f &lt;location of the file e.g. ./Modelfile&gt;</code></li> <li><code>ollama run choose-a-model-name</code></li> <li>Start using the model!</li> </ol> <p>To view the Modelfile of a given model, use the <code>ollama show --modelfile</code> command.</p> <pre><code>ollama show --modelfile llama3.2\n</code></pre> <p>Output:</p> <pre><code># Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llama3.2:latest\nFROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29\nTEMPLATE \"\"\"{{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{{ .System }}&lt;|eot_id|&gt;{{ end }}{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n{{ .Response }}&lt;|eot_id|&gt;\"\"\"\nPARAMETER stop \"&lt;|start_header_id|&gt;\"\nPARAMETER stop \"&lt;|end_header_id|&gt;\"\nPARAMETER stop \"&lt;|eot_id|&gt;\"\nPARAMETER stop \"&lt;|reserved_special_token\"\n</code></pre>"},{"location":"modelfile/#instructions","title":"Instructions","text":""},{"location":"modelfile/#from-required","title":"FROM (Required)","text":"<p>The <code>FROM</code> instruction defines the base model to use when creating a model.</p> <pre><code>FROM &lt;model name&gt;:&lt;tag&gt;\n</code></pre>"},{"location":"modelfile/#build-from-existing-model","title":"Build from existing model","text":"<pre><code>FROM llama3.2\n</code></pre> <p>A list of available base models: https://github.com/ollama/ollama#model-library Additional models can be found at: https://ollama.com/library</p>"},{"location":"modelfile/#build-from-a-safetensors-model","title":"Build from a Safetensors model","text":"<pre><code>FROM &lt;model directory&gt;\n</code></pre> <p>The model directory should contain the Safetensors weights for a supported architecture.</p> <p>Currently supported model architectures:   * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2)   * Mistral (including Mistral 1, Mistral 2, and Mixtral)   * Gemma (including Gemma 1 and Gemma 2)   * Phi3</p>"},{"location":"modelfile/#build-from-a-gguf-file","title":"Build from a GGUF file","text":"<pre><code>FROM ./ollama-model.gguf\n</code></pre> <p>The GGUF file location should be specified as an absolute path or relative to the <code>Modelfile</code> location.</p>"},{"location":"modelfile/#parameter","title":"PARAMETER","text":"<p>The <code>PARAMETER</code> instruction defines a parameter that can be set when the model is run.</p> <pre><code>PARAMETER &lt;parameter&gt; &lt;parametervalue&gt;\n</code></pre>"},{"location":"modelfile/#valid-parameters-and-values","title":"Valid Parameters and Values","text":"Parameter Description Value Type Example Usage num_ctx Sets the size of the context window used to generate the next token. (Default: 2048) int num_ctx 4096 repeat_last_n Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx) int repeat_last_n 64 repeat_penalty Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1) float repeat_penalty 1.1 temperature The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8) float temperature 0.7 seed Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0) int seed 42 stop Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate <code>stop</code> parameters in a modelfile. string stop \"AI assistant:\" num_predict Maximum number of tokens to predict when generating text. (Default: -1, infinite generation) int num_predict 42 top_k Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40) int top_k 40 top_p Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9) float top_p 0.9 min_p Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter p represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with p=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0) float min_p 0.05"},{"location":"modelfile/#template","title":"TEMPLATE","text":"<p><code>TEMPLATE</code> of the full prompt template to be passed into the model. It may include (optionally) a system message, a user's message and the response from the model. Note: syntax may be model specific. Templates use Go template syntax.</p>"},{"location":"modelfile/#template-variables","title":"Template Variables","text":"Variable Description <code>{{ .System }}</code> The system message used to specify custom behavior. <code>{{ .Prompt }}</code> The user prompt message. <code>{{ .Response }}</code> The response from the model. When generating a response, text after this variable is omitted. <pre><code>TEMPLATE \"\"\"{{ if .System }}&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n{{ end }}{{ if .Prompt }}&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n{{ end }}&lt;|im_start|&gt;assistant\n\"\"\"\n</code></pre>"},{"location":"modelfile/#system","title":"SYSTEM","text":"<p>The <code>SYSTEM</code> instruction specifies the system message to be used in the template, if applicable.</p> <pre><code>SYSTEM \"\"\"&lt;system message&gt;\"\"\"\n</code></pre>"},{"location":"modelfile/#adapter","title":"ADAPTER","text":"<p>The <code>ADAPTER</code> instruction specifies a fine tuned LoRA adapter that should apply to the base model. The value of the adapter should be an absolute path or a path relative to the Modelfile. The base model should be specified with a <code>FROM</code> instruction. If the base model is not the same as the base model that the adapter was tuned from the behaviour will be erratic.</p>"},{"location":"modelfile/#safetensor-adapter","title":"Safetensor adapter","text":"<pre><code>ADAPTER &lt;path to safetensor adapter&gt;\n</code></pre> <p>Currently supported Safetensor adapters:   * Llama (including Llama 2, Llama 3, and Llama 3.1)   * Mistral (including Mistral 1, Mistral 2, and Mixtral)   * Gemma (including Gemma 1 and Gemma 2)</p>"},{"location":"modelfile/#gguf-adapter","title":"GGUF adapter","text":"<pre><code>ADAPTER ./ollama-lora.gguf\n</code></pre>"},{"location":"modelfile/#license","title":"LICENSE","text":"<p>The <code>LICENSE</code> instruction allows you to specify the legal license under which the model used with this Modelfile is shared or distributed.</p> <pre><code>LICENSE \"\"\"\n&lt;license text&gt;\n\"\"\"\n</code></pre>"},{"location":"modelfile/#message","title":"MESSAGE","text":"<p>The <code>MESSAGE</code> instruction allows you to specify a message history for the model to use when responding. Use multiple iterations of the MESSAGE command to build up a conversation which will guide the model to answer in a similar way.</p> <pre><code>MESSAGE &lt;role&gt; &lt;message&gt;\n</code></pre>"},{"location":"modelfile/#valid-roles","title":"Valid roles","text":"Role Description system Alternate way of providing the SYSTEM message for the model. user An example message of what the user could have asked. assistant An example message of how the model should respond."},{"location":"modelfile/#example-conversation","title":"Example conversation","text":"<pre><code>MESSAGE user Is Toronto in Canada?\nMESSAGE assistant yes\nMESSAGE user Is Sacramento in Canada?\nMESSAGE assistant no\nMESSAGE user Is Ontario in Canada?\nMESSAGE assistant yes\n</code></pre>"},{"location":"modelfile/#notes","title":"Notes","text":"<ul> <li>the <code>Modelfile</code> is not case sensitive. In the examples, uppercase instructions are used to make it easier to distinguish it from arguments.</li> <li>Instructions can be in any order. In the examples, the <code>FROM</code> instruction is first to keep it easily readable.</li> </ul>"},{"location":"openai/","title":"OpenAI compatibility","text":"<p>Note</p> <p>OpenAI compatibility is experimental and is subject to major adjustments including breaking changes. For fully-featured access to the Ollama API, see the Ollama Python library, JavaScript library and REST API.</p> <p>Ollama provides experimental compatibility with parts of the OpenAI API to help connect existing applications to Ollama.</p>"},{"location":"openai/#usage","title":"Usage","text":""},{"location":"openai/#openai-python-library","title":"OpenAI Python library","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url='http://localhost:11434/v1/',\n\n    # required but ignored\n    api_key='ollama',\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            'role': 'user',\n            'content': 'Say this is a test',\n        }\n    ],\n    model='llama3.2',\n)\n\nresponse = client.chat.completions.create(\n    model=\"llava\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"data:image/png;base64,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\",\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\ncompletion = client.completions.create(\n    model=\"llama3.2\",\n    prompt=\"Say this is a test\",\n)\n\nlist_completion = client.models.list()\n\nmodel = client.models.retrieve(\"llama3.2\")\n\nembeddings = client.embeddings.create(\n    model=\"all-minilm\",\n    input=[\"why is the sky blue?\", \"why is the grass green?\"],\n)\n</code></pre>"},{"location":"openai/#structured-outputs","title":"Structured outputs","text":"<pre><code>from pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\n# Define the schema for the response\nclass FriendInfo(BaseModel):\n    name: str\n    age: int \n    is_available: bool\n\nclass FriendList(BaseModel):\n    friends: list[FriendInfo]\n\ntry:\n    completion = client.beta.chat.completions.parse(\n        temperature=0,\n        model=\"llama3.1:8b\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format\"}\n        ],\n        response_format=FriendList,\n    )\n\n    friends_response = completion.choices[0].message\n    if friends_response.parsed:\n        print(friends_response.parsed)\n    elif friends_response.refusal:\n        print(friends_response.refusal)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"openai/#openai-javascript-library","title":"OpenAI JavaScript library","text":"<pre><code>import OpenAI from 'openai'\n\nconst openai = new OpenAI({\n  baseURL: 'http://localhost:11434/v1/',\n\n  // required but ignored\n  apiKey: 'ollama',\n})\n\nconst chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'llama3.2',\n})\n\nconst response = await openai.chat.completions.create({\n    model: \"llava\",\n    messages: [\n        {\n        role: \"user\",\n        content: [\n            { type: \"text\", text: \"What's in this image?\" },\n            {\n            type: \"image_url\",\n            image_url: \"data:image/png;base64,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\",\n            },\n        ],\n        },\n    ],\n})\n\nconst completion = await openai.completions.create({\n    model: \"llama3.2\",\n    prompt: \"Say this is a test.\",\n})\n\nconst listCompletion = await openai.models.list()\n\nconst model = await openai.models.retrieve(\"llama3.2\")\n\nconst embedding = await openai.embeddings.create({\n  model: \"all-minilm\",\n  input: [\"why is the sky blue?\", \"why is the grass green?\"],\n})\n</code></pre>"},{"location":"openai/#curl","title":"<code>curl</code>","text":"<pre><code>curl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama3.2\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n\ncurl http://localhost:11434/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llava\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What'\\''s in this image?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n               \"url\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717927ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6eT8h4ni/iIWpR5lPyA6ETkNXoSukvpJAD3AsXLiwpZs49+fPn5ke4j10TqYvegSfn0OnafC+Tv9ooA/JPkgQysqQNBzagXY55nO/oa1F7qvIPWkRL12WRpMWUvpVDYmxAPehxWSe8ZEXL20sadYIozfmNch4QJPAfeJgW3rNsnzphBKNJM2KKODo1rVOMRYik5ETy3ix4qWNI81qAAirizgMIc+yhTytx0JWZuNI03qsrgWlGtwjoS9XwgUhWGyhUaRZZQNNIEwCiXD16tXcAHUs79co0vSD8rrJCIW98pzvxpAWyyo3HYwqS0+H0BjStClcZJT5coMm6D2LOF8TolGJtK9fvyZpyiC5ePFi9nc/oJU4eiEP0jVoAnHa9wyJycITMP78+eMeP37sXrx44d6+fdt6f82aNdkx1pg9e3Zb5W+RSRE+n+VjksQWifvVaTKFhn5O8my63K8Qabdv33b379/PiAP//vuvW7BggZszZ072/+TJk91YgkafPn166zXB1rQHFvouAWHq9z3SEevSUerqCn2/dDCeta2jxYbr69evk4MHDyY7d+7MjhMnTiTPnz9Pfv/+nfQT2ggpO2dMF8cghuoM7Ygj5iWCqRlGFml0QC/ftGmTmzt3rmsaKDsgBSPh0/8yPeLLBihLkOKJc0jp8H8vUzcxIA1k6QJ/c78tWEyj5P3o4u9+jywNPdJi5rAH9x0KHcl4Hg570eQp3+vHXGyrmEeigzQsQsjavXt38ujRo44LQuDDhw+TW7duRS1HGgMxhNXHgflaNTOsHyKvHK5Ijo2jbFjJBQK9YwFd6RVMzfgRBmEfP37suBBm/p49e1qjEP2mwTViNRo0VJWH1deMXcNK08uUjVUu7s/zRaL+oLNxz1bpANco4npUgX4G2eFbpDFyQoQxojBCpEGSytmOH8qrH5Q9vuzD6ofQylkCUmh8DBAr+q8JCyVNtWQIidKQE9wNtLSQnS4jDSsxNHogzFuQBw4cyM61UKVsjfr3ooBkPSqqQHesUPWVtzi9/vQi1T+rJj7WiTz4Pt/l3LxUkr5P2VYZaZ4URpsE+st/dujQoaBBYokbrz/8TJNQYLSonrPS9kUaSkPeZyj1AWSj+d+VBoy1pIWVNed8P0Ll/ee5HdGRhrHhR5GGN0r4LGZBaj8oFDJitBTJzIZgFcmU0Y8ytWMZMzJOaXUSrUs5RxKnrxmbb5YXO9VGUhtpXldhEUogFr3IzIsvlpmdosVcGVGXFWp2oU9kLFL3dEkSz6NHEY1sjSRdIuDFWEhd8KxFqsRi1uM/nz9/zpxnwlESONdg6dKlbsaMGS4EHFHtjFIDHwKOo46l4TxSuxgDzi+rE2jg+BaFruOX4HXa0Nnf1lwAPufZeF8/r6zD97WK2qFnGjBxTw5qNGPxT+5T/r7/7RawFC3j4vTp09koCxkeHjqbHJqArmH5UrFKKksnxrK7FuRIs8STfBZv+luugXZ2pR/pP9Ois4z+TiMzUUkUjD0iEi1fzX8GmXyuxUBRcaUfykV0YZnlJGKQpOiGB76x5GeWkWWJc3mOrK6S7xdND+W5N6XyaRgtWJFe13GkaZnKOsYqGdOVVVbGupsyA/l7emTLHi7vwTdirNEt0qxnzAvBFcnQF16xh/TMpUuXHDowhlA9vQVraQhkudRdzOnK+04ZSP3DUhVSP61YsaLtd/ks7ZgtPcXqPqEafHkdqa84X6aCeL7YWlv6edGFHb+ZFICPlljHhg0bKuk0CSvVznWsotRu433alNdFrqG45ejoaPCaUkWERpLXjzFL2Rpllp7PJU2a/v7Ab8N05/9t27Z16KUqoFGsxnI9EosS2niSYg9SpU6B4JgTrvVW1flt1sT+0ADIJU2maXzcUTraGCRaL1Wp9rUMk16PMom8QhruxzvZIegJjFU7LLCePfS8uaQdPny4jTTL0dbee5mYokQsXTIWNY46kuMbnt8Kmec+LGWtOVIl9cT1rCB0V8WqkjAsRwta93TbwNYoGKsUSChN44lgBNCoHLHzquYKrU6qZ8lolCIN0Rh6cP0Q3U6I6IXILYOQI513hJaSKAorFpuHXJNfVlpRtmYBk1Su1obZr5dnKAO+L10Hrj3WZW+E3qh6IszE37F6EB+68mGpvKm4eb9bFrlzrok7fvr0Kfv727dvWRmdVTJHw0qiiCUSZ6wCK+7XL/AcsgNyL74DQQ730sv78Su7+t/A36MdY0sW5o40ahslXr58aZ5HtZB8GH64m9EmMZ7FpYw4T6QnrZfgenrhFxaSiSGXtPnz57e9TkNZLvTjeqhr734CNtrK41L40sUQckmj1lGKQ0rC37x544r8eNXRpnVE3ZZY7zXo8NomiO0ZUCj2uHz58rbXoZ6gc0uA+F6ZeKS/jhRDUq8MKrTho9fEkihMmhxtBI1DxKFY9XLpVcSkfoi8JGnToZO5sU5aiDQIW716ddt7ZLYtMQlhECdBGXZZMWldY5BHm5xgAroWj4C0hbYkSc/jBmggIrXJWlZM6pSETsEPGqZOndr2uuuR5rF169a2HoHPdurUKZM4CO1WTPqaDaAd+GFGKdIQkxAn9RuEWcTRyN2KSUgiSgF5aWzPTeA/lN5rZubMmR2bE4SIC4nJoltgAV/dVefZm72AtctUCJU2CMJ327hxY9t7EHbkyJFseq+EJSY16RPo3Dkq1kkr7+q0bNmyDuLQcZBEPYmHVdOBiJyIlrRDq41YPWfXOxUysi5fvtyaj+2BpcnsUV/oSoEMOk2CQGlr4ckhBwaetBhjCwH0ZHtJROPJkyc7UjcYLDjmrH7ADTEBXFfOYmB0k9oYBOjJ8b4aOYSe7QkKcYhFlq3QYLQhSidNmtS2RATwy8YOM3EQJsUjKiaWZ+vZToUQgzhkHXudb/PW5YMHD9yZM2faPsMwoc7RciYJXbGuBqJ1UIGKKLv915jsvgtJxCZDubdXr165mzdvtr1Hz5LONA8jrUwKPqsmVesKa49S3Q4WxmRPUEYdTjgiUcfUwLx589ySJUva3oMkP6IYddq6HMS4o55xBJBUeRjzfa4Zdeg56QZ43LhxoyPo7Lf1kNt7oO8wWAbNwaYjIv5lhyS7kRf96dvm5Jah8vfvX3flyhX35cuX6HfzFHOToS1H4BenCaHvO8pr8iDuwoUL7tevX+b5ZdbBair0xkFIlFDlW4ZknEClsp/TzXyAKVOmmHWFVSbDNw1l1+4f90U6IY/q4V27dpnE9bJ+v87QEydjqx/UamVVPRG+mwkNTYN+9tjkwzEx+atCm/X9WvWtDtAb68Wy9LXa1UmvCDDIpPkyOQ5ZwSzJ4jMrvFcr0rSjOUh+GcT4LSg5ugkW1Io0/SCDQBojh0hPlaJdah+tkVYrnTZowP8iq1F1TgMBBauufyB33x1v+NWFYmT5KmppgHC+NkAgbmRkpD3yn9QIseXymoTQFGQmIOKTxiZIWpvAatenVqRVXf2nTrAWMsPnKrMZHz6bJq5jvce6QK8J1cQNgKxlJapMPdZSR64/UivS9NztpkVEdKcrs5alhhWP9NeqlfWopzhZScI6QxseegZRGeg5a8C3Re1Mfl1ScP36ddcUaMuv24iOJtz7sbUjTS4qBvKmstYJoUauiuD3k5qhyr7QdUHMeCgLa1Ear9NquemdXgmum4fvJ6w1lqsuDhNrg1qSpleJK7K3TF0Q2jSd94uSZ60kK1e3qyVpQK6PVWXp2/FC3mp6jBhKKOiY2h3gtUV64TWM6wDETRPLDfSakXmH3w8g9Jlug8ZtTt4kVF0kLUYYmCCtD/DrQ5YhMGbA9L3ucdjh0y8kOHW5gU/VEEmJTcL4Pz/f7mgoAbYkAAAAAElFTkSuQmCC\"\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 300\n  }'\n\ncurl http://localhost:11434/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama3.2\",\n        \"prompt\": \"Say this is a test\"\n    }'\n\ncurl http://localhost:11434/v1/models\n\ncurl http://localhost:11434/v1/models/llama3.2\n\ncurl http://localhost:11434/v1/embeddings \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"all-minilm\",\n        \"input\": [\"why is the sky blue?\", \"why is the grass green?\"]\n    }'\n</code></pre>"},{"location":"openai/#endpoints","title":"Endpoints","text":""},{"location":"openai/#v1chatcompletions","title":"<code>/v1/chat/completions</code>","text":""},{"location":"openai/#supported-features","title":"Supported features","text":"<ul> <li> Chat completions</li> <li> Streaming</li> <li> JSON mode</li> <li> Reproducible outputs</li> <li> Vision</li> <li> Tools</li> <li> Logprobs</li> </ul>"},{"location":"openai/#supported-request-fields","title":"Supported request fields","text":"<ul> <li> <code>model</code></li> <li> <code>messages</code></li> <li> Text <code>content</code></li> <li> Image <code>content</code><ul> <li> Base64 encoded image</li> <li> Image URL</li> </ul> </li> <li> Array of <code>content</code> parts</li> <li> <code>frequency_penalty</code></li> <li> <code>presence_penalty</code></li> <li> <code>response_format</code></li> <li> <code>seed</code></li> <li> <code>stop</code></li> <li> <code>stream</code></li> <li> <code>stream_options</code></li> <li> <code>include_usage</code></li> <li> <code>temperature</code></li> <li> <code>top_p</code></li> <li> <code>max_tokens</code></li> <li> <code>tools</code></li> <li> <code>tool_choice</code></li> <li> <code>logit_bias</code></li> <li> <code>user</code></li> <li> <code>n</code></li> </ul>"},{"location":"openai/#v1completions","title":"<code>/v1/completions</code>","text":""},{"location":"openai/#supported-features_1","title":"Supported features","text":"<ul> <li> Completions</li> <li> Streaming</li> <li> JSON mode</li> <li> Reproducible outputs</li> <li> Logprobs</li> </ul>"},{"location":"openai/#supported-request-fields_1","title":"Supported request fields","text":"<ul> <li> <code>model</code></li> <li> <code>prompt</code></li> <li> <code>frequency_penalty</code></li> <li> <code>presence_penalty</code></li> <li> <code>seed</code></li> <li> <code>stop</code></li> <li> <code>stream</code></li> <li> <code>stream_options</code></li> <li> <code>include_usage</code></li> <li> <code>temperature</code></li> <li> <code>top_p</code></li> <li> <code>max_tokens</code></li> <li> <code>suffix</code></li> <li> <code>best_of</code></li> <li> <code>echo</code></li> <li> <code>logit_bias</code></li> <li> <code>user</code></li> <li> <code>n</code></li> </ul>"},{"location":"openai/#notes","title":"Notes","text":"<ul> <li><code>prompt</code> currently only accepts a string</li> </ul>"},{"location":"openai/#v1models","title":"<code>/v1/models</code>","text":""},{"location":"openai/#notes_1","title":"Notes","text":"<ul> <li><code>created</code> corresponds to when the model was last modified</li> <li><code>owned_by</code> corresponds to the ollama username, defaulting to <code>\"library\"</code></li> </ul>"},{"location":"openai/#v1modelsmodel","title":"<code>/v1/models/{model}</code>","text":""},{"location":"openai/#notes_2","title":"Notes","text":"<ul> <li><code>created</code> corresponds to when the model was last modified</li> <li><code>owned_by</code> corresponds to the ollama username, defaulting to <code>\"library\"</code></li> </ul>"},{"location":"openai/#v1embeddings","title":"<code>/v1/embeddings</code>","text":""},{"location":"openai/#supported-request-fields_2","title":"Supported request fields","text":"<ul> <li> <code>model</code></li> <li> <code>input</code></li> <li> string</li> <li> array of strings</li> <li> array of tokens</li> <li> array of token arrays</li> <li> <code>encoding format</code></li> <li> <code>dimensions</code></li> <li> <code>user</code></li> </ul>"},{"location":"openai/#models","title":"Models","text":"<p>Before using a model, pull it locally <code>ollama pull</code>:</p> <pre><code>ollama pull llama3.2\n</code></pre>"},{"location":"openai/#default-model-names","title":"Default model names","text":"<p>For tooling that relies on default OpenAI model names such as <code>gpt-3.5-turbo</code>, use <code>ollama cp</code> to copy an existing model name to a temporary name:</p> <pre><code>ollama cp llama3.2 gpt-3.5-turbo\n</code></pre> <p>Afterwards, this new model name can be specified the <code>model</code> field:</p> <pre><code>curl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"gpt-3.5-turbo\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n</code></pre>"},{"location":"openai/#setting-the-context-size","title":"Setting the context size","text":"<p>The OpenAI API does not have a way of setting the context size for a model. If you need to change the context size, create a <code>Modelfile</code> which looks like:</p> <pre><code>FROM &lt;some model&gt;\nPARAMETER num_ctx &lt;context size&gt;\n</code></pre> <p>Use the <code>ollama create mymodel</code> command to create a new model with the updated context size. Call the API with the updated model name:</p> <pre><code>curl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"mymodel\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n</code></pre>"},{"location":"template/","title":"Template","text":"<p>Ollama provides a powerful templating engine backed by Go's built-in templating engine to construct prompts for your large language model. This feature is a valuable tool to get the most out of your models.</p>"},{"location":"template/#basic-template-structure","title":"Basic Template Structure","text":"<p>A basic Go template consists of three main parts:</p> <ul> <li>Layout: The overall structure of the template.</li> <li>Variables: Placeholders for dynamic data that will be replaced with actual values when the template is rendered.</li> <li>Functions: Custom functions or logic that can be used to manipulate the template's content.</li> </ul> <p>Here's an example of a simple chat template:</p> <pre><code>{{- range .Messages }}\n{{ .Role }}: {{ .Content }}\n{{- end }}\n</code></pre> <p>In this example, we have:</p> <ul> <li>A basic messages structure (layout)</li> <li>Three variables: <code>Messages</code>, <code>Role</code>, and <code>Content</code> (variables)</li> <li>A custom function (action) that iterates over an array of items (<code>range .Messages</code>) and displays each item</li> </ul>"},{"location":"template/#adding-templates-to-your-model","title":"Adding templates to your model","text":"<p>By default, models imported into Ollama have a default template of <code>{{ .Prompt }}</code>, i.e. user inputs are sent verbatim to the LLM. This is appropriate for text or code completion models but lacks essential markers for chat or instruction models.</p> <p>Omitting a template in these models puts the responsibility of correctly templating input onto the user. Adding a template allows users to easily get the best results from the model.</p> <p>To add templates in your model, you'll need to add a <code>TEMPLATE</code> command to the Modelfile. Here's an example using Meta's Llama 3.</p> <pre><code>FROM llama3.2\n\nTEMPLATE \"\"\"{{- if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{{ .System }}&lt;|eot_id|&gt;\n{{- end }}\n{{- range .Messages }}&lt;|start_header_id|&gt;{{ .Role }}&lt;|end_header_id|&gt;\n\n{{ .Content }}&lt;|eot_id|&gt;\n{{- end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n\"\"\"\n</code></pre>"},{"location":"template/#variables","title":"Variables","text":"<p><code>System</code> (string): system prompt</p> <p><code>Prompt</code> (string): user prompt</p> <p><code>Response</code> (string): assistant response</p> <p><code>Suffix</code> (string): text inserted after the assistant's response</p> <p><code>Messages</code> (list): list of messages</p> <p><code>Messages[].Role</code> (string): role which can be one of <code>system</code>, <code>user</code>, <code>assistant</code>, or <code>tool</code></p> <p><code>Messages[].Content</code> (string):  message content</p> <p><code>Messages[].ToolCalls</code> (list): list of tools the model wants to call</p> <p><code>Messages[].ToolCalls[].Function</code> (object): function to call</p> <p><code>Messages[].ToolCalls[].Function.Name</code> (string): function name</p> <p><code>Messages[].ToolCalls[].Function.Arguments</code> (map): mapping of argument name to argument value</p> <p><code>Tools</code> (list): list of tools the model can access</p> <p><code>Tools[].Type</code> (string): schema type. <code>type</code> is always <code>function</code></p> <p><code>Tools[].Function</code> (object): function definition</p> <p><code>Tools[].Function.Name</code> (string): function name</p> <p><code>Tools[].Function.Description</code> (string): function description</p> <p><code>Tools[].Function.Parameters</code> (object): function parameters</p> <p><code>Tools[].Function.Parameters.Type</code> (string): schema type. <code>type</code> is always <code>object</code></p> <p><code>Tools[].Function.Parameters.Required</code> (list): list of required properties</p> <p><code>Tools[].Function.Parameters.Properties</code> (map): mapping of property name to property definition</p> <p><code>Tools[].Function.Parameters.Properties[].Type</code> (string): property type</p> <p><code>Tools[].Function.Parameters.Properties[].Description</code> (string): property description</p> <p><code>Tools[].Function.Parameters.Properties[].Enum</code> (list): list of valid values</p>"},{"location":"template/#tips-and-best-practices","title":"Tips and Best Practices","text":"<p>Keep the following tips and best practices in mind when working with Go templates:</p> <ul> <li>Be mindful of dot: Control flow structures like <code>range</code> and <code>with</code> changes the value <code>.</code></li> <li>Out-of-scope variables: Use <code>$.</code> to reference variables not currently in scope, starting from the root</li> <li>Whitespace control: Use <code>-</code> to trim leading (<code>{{-</code>) and trailing (<code>-}}</code>) whitespace</li> </ul>"},{"location":"template/#examples","title":"Examples","text":""},{"location":"template/#example-messages","title":"Example Messages","text":""},{"location":"template/#chatml","title":"ChatML","text":"<p>ChatML is a popular template format. It can be used for models such as Databrick's DBRX, Intel's Neural Chat, and Microsoft's Orca 2.</p> <pre><code>{{- range .Messages }}&lt;|im_start|&gt;{{ .Role }}\n{{ .Content }}&lt;|im_end|&gt;\n{{ end }}&lt;|im_start|&gt;assistant\n</code></pre>"},{"location":"template/#example-tools","title":"Example Tools","text":"<p>Tools support can be added to a model by adding a <code>{{ .Tools }}</code> node to the template. This feature is useful for models trained to call external tools and can a powerful tool for retrieving real-time data or performing complex tasks.</p>"},{"location":"template/#mistral","title":"Mistral","text":"<p>Mistral v0.3 and Mixtral 8x22B supports tool calling.</p> <pre><code>{{- range $index, $_ := .Messages }}\n{{- if eq .Role \"user\" }}\n{{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS] {{ json $.Tools }}[/AVAILABLE_TOOLS]\n{{- end }}[INST] {{ if and (eq (len (slice $.Messages $index)) 1) $.System }}{{ $.System }}\n\n{{ end }}{{ .Content }}[/INST]\n{{- else if eq .Role \"assistant\" }}\n{{- if .Content }} {{ .Content }}&lt;/s&gt;\n{{- else if .ToolCalls }}[TOOL_CALLS] [\n{{- range .ToolCalls }}{\"name\": \"{{ .Function.Name }}\", \"arguments\": {{ json .Function.Arguments }}}\n{{- end }}]&lt;/s&gt;\n{{- end }}\n{{- else if eq .Role \"tool\" }}[TOOL_RESULTS] {\"content\": {{ .Content }}}[/TOOL_RESULTS]\n{{- end }}\n{{- end }}\n</code></pre>"},{"location":"template/#example-fill-in-middle","title":"Example Fill-in-Middle","text":"<p>Fill-in-middle support can be added to a model by adding a <code>{{ .Suffix }}</code> node to the template. This feature is useful for models that are trained to generate text in the middle of user input, such as code completion models.</p>"},{"location":"template/#codellama","title":"CodeLlama","text":"<p>CodeLlama 7B and 13B code completion models support fill-in-middle.</p> <pre><code>&lt;PRE&gt; {{ .Prompt }} &lt;SUF&gt;{{ .Suffix }} &lt;MID&gt;\n</code></pre> <p>Note</p> <p>CodeLlama 34B and 70B code completion and all instruct and Python fine-tuned models do not support fill-in-middle.</p>"},{"location":"template/#codestral","title":"Codestral","text":"<p>Codestral 22B supports fill-in-middle.</p> <pre><code>[SUFFIX]{{ .Suffix }}[PREFIX] {{ .Prompt }}\n</code></pre>"},{"location":"troubleshooting/","title":"How to troubleshoot issues","text":"<p>Sometimes Ollama may not perform as expected. One of the best ways to figure out what happened is to take a look at the logs. Find the logs on Mac by running the command:</p> <pre><code>cat ~/.ollama/logs/server.log\n</code></pre> <p>On Linux systems with systemd, the logs can be found with this command:</p> <pre><code>journalctl -u ollama --no-pager --follow --pager-end \n</code></pre> <p>When you run Ollama in a container, the logs go to stdout/stderr in the container:</p> <pre><code>docker logs &lt;container-name&gt;\n</code></pre> <p>(Use <code>docker ps</code> to find the container name)</p> <p>If manually running <code>ollama serve</code> in a terminal, the logs will be on that terminal.</p> <p>When you run Ollama on Windows, there are a few different locations. You can view them in the explorer window by hitting <code>&lt;cmd&gt;+R</code> and type in: - <code>explorer %LOCALAPPDATA%\\Ollama</code> to view logs.  The most recent server logs will be in <code>server.log</code> and older logs will be in <code>server-#.log</code>  - <code>explorer %LOCALAPPDATA%\\Programs\\Ollama</code> to browse the binaries (The installer adds this to your user PATH) - <code>explorer %HOMEPATH%\\.ollama</code> to browse where models and configuration is stored</p> <p>To enable additional debug logging to help troubleshoot problems, first Quit the running app from the tray menu then in a powershell terminal</p> <pre><code>$env:OLLAMA_DEBUG=\"1\"\n&amp; \"ollama app.exe\"\n</code></pre> <p>Join the Discord for help interpreting the logs.</p>"},{"location":"troubleshooting/#llm-libraries","title":"LLM libraries","text":"<p>Ollama includes multiple LLM libraries compiled for different GPUs and CPU vector features. Ollama tries to pick the best one based on the capabilities of your system. If this autodetection has problems, or you run into other problems (e.g. crashes in your GPU) you can workaround this by forcing a specific LLM library. <code>cpu_avx2</code> will perform the best, followed by <code>cpu_avx</code> an the slowest but most compatible is <code>cpu</code>. Rosetta emulation under MacOS will work with the <code>cpu</code> library. </p> <p>In the server log, you will see a message that looks something like this (varies from release to release):</p> <pre><code>Dynamic LLM libraries [rocm_v6 cpu cpu_avx cpu_avx2 cuda_v12 rocm_v5]\n</code></pre> <p>Experimental LLM Library Override</p> <p>You can set OLLAMA_LLM_LIBRARY to any of the available LLM libraries to bypass autodetection, so for example, if you have a CUDA card, but want to force the CPU LLM library with AVX2 vector support, use:</p> <pre><code>OLLAMA_LLM_LIBRARY=\"cpu_avx2\" ollama serve\n</code></pre> <p>You can see what features your CPU has with the following.</p> <pre><code>cat /proc/cpuinfo| grep flags | head -1\n</code></pre>"},{"location":"troubleshooting/#installing-older-or-pre-release-versions-on-linux","title":"Installing older or pre-release versions on Linux","text":"<p>If you run into problems on Linux and want to install an older version, or you'd like to try out a pre-release before it's officially released, you can tell the install script which version to install.</p> <pre><code>curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh\n</code></pre>"},{"location":"troubleshooting/#linux-docker","title":"Linux docker","text":"<p>If Ollama initially works on the GPU in a docker container, but then switches to running on CPU after some period of time with errors in the server log reporting GPU discovery failures, this can be resolved by disabling systemd cgroup management in Docker.  Edit <code>/etc/docker/daemon.json</code> on the host and add <code>\"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]</code> to the docker configuration.</p>"},{"location":"troubleshooting/#nvidia-gpu-discovery","title":"NVIDIA GPU Discovery","text":"<p>When Ollama starts up, it takes inventory of the GPUs present in the system to determine compatibility and how much VRAM is available.  Sometimes this discovery can fail to find your GPUs.  In general, running the latest driver will yield the best results.</p>"},{"location":"troubleshooting/#linux-nvidia-troubleshooting","title":"Linux NVIDIA Troubleshooting","text":"<p>If you are using a container to run Ollama, make sure you've set up the container runtime first as described in docker.md</p> <p>Sometimes the Ollama can have difficulties initializing the GPU. When you check the server logs, this can show up as various error codes, such as \"3\" (not initialized), \"46\" (device unavailable), \"100\" (no device), \"999\" (unknown), or others. The following troubleshooting techniques may help resolve the problem</p> <ul> <li>If you are using a container, is the container runtime working?  Try <code>docker run --gpus all ubuntu nvidia-smi</code> - if this doesn't work, Ollama won't be able to see your NVIDIA GPU.</li> <li>Is the uvm driver loaded? <code>sudo nvidia-modprobe -u</code></li> <li>Try reloading the nvidia_uvm driver - <code>sudo rmmod nvidia_uvm</code> then <code>sudo modprobe nvidia_uvm</code></li> <li>Try rebooting</li> <li>Make sure you're running the latest nvidia drivers</li> </ul> <p>If none of those resolve the problem, gather additional information and file an issue: - Set <code>CUDA_ERROR_LEVEL=50</code> and try again to get more diagnostic logs - Check dmesg for any errors <code>sudo dmesg | grep -i nvrm</code> and <code>sudo dmesg | grep -i nvidia</code></p>"},{"location":"troubleshooting/#amd-gpu-discovery","title":"AMD GPU Discovery","text":"<p>On linux, AMD GPU access typically requires <code>video</code> and/or <code>render</code> group membership to access the <code>/dev/kfd</code> device.  If permissions are not set up correctly, Ollama will detect this and report an error in the server log.</p> <p>When running in a container, in some Linux distributions and container runtimes, the ollama process may be unable to access the GPU.  Use <code>ls -lnd /dev/kfd /dev/dri /dev/dri/*</code> on the host system to determine the numeric group IDs on your system, and pass additional <code>--group-add ...</code> arguments to the container so it can access the required devices.   For example, in the following output <code>crw-rw---- 1 0  44 226,   0 Sep 16 16:55 /dev/dri/card0</code> the group ID column is <code>44</code> </p> <p>If you are experiencing problems getting Ollama to correctly discover or use your GPU for inference, the following may help isolate the failure. - <code>AMD_LOG_LEVEL=3</code> Enable info log levels in the AMD HIP/ROCm libraries.  This can help show more detailed error codes that can help troubleshoot problems - <code>OLLAMA_DEBUG=1</code> During GPU discovery additional information will be reported - Check dmesg for any errors from amdgpu or kfd drivers <code>sudo dmesg | grep -i amdgpu</code> and <code>sudo dmesg | grep -i kfd</code></p>"},{"location":"troubleshooting/#multiple-amd-gpus","title":"Multiple AMD GPUs","text":"<p>If you experience gibberish responses when models load across multiple AMD GPUs on Linux, see the following guide.</p> <ul> <li>https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/mgpu.html#mgpu-known-issues-and-limitations</li> </ul>"},{"location":"troubleshooting/#windows-terminal-errors","title":"Windows Terminal Errors","text":"<p>Older versions of Windows 10 (e.g., 21H1) are known to have a bug where the standard terminal program does not display control characters correctly.  This can result in a long string of strings like <code>\u2190[?25h\u2190[?25l</code> being displayed, sometimes erroring with <code>The parameter is incorrect</code>  To resolve this problem, please update to Win 10 22H1 or newer.</p>"},{"location":"windows/","title":"Ollama Windows","text":"<p>Welcome to Ollama for Windows.</p> <p>No more WSL required!</p> <p>Ollama now runs as a native Windows application, including NVIDIA and AMD Radeon GPU support. After installing Ollama for Windows, Ollama will run in the background and the <code>ollama</code> command line is available in <code>cmd</code>, <code>powershell</code> or your favorite terminal application. As usual the Ollama api will be served on <code>http://localhost:11434</code>.</p>"},{"location":"windows/#system-requirements","title":"System Requirements","text":"<ul> <li>Windows 10 22H2 or newer, Home or Pro</li> <li>NVIDIA 452.39 or newer Drivers if you have an NVIDIA card</li> <li>AMD Radeon Driver https://www.amd.com/en/support if you have a Radeon card</li> </ul> <p>Ollama uses unicode characters for progress indication, which may render as unknown squares in some older terminal fonts in Windows 10. If you see this, try changing your terminal font settings.</p>"},{"location":"windows/#filesystem-requirements","title":"Filesystem Requirements","text":"<p>The Ollama install does not require Administrator, and installs in your home directory by default.  You'll need at least 4GB of space for the binary install.  Once you've installed Ollama, you'll need additional space for storing the Large Language models, which can be tens to hundreds of GB in size.  If your home directory doesn't have enough space, you can change where the binaries are installed, and where the models are stored.</p>"},{"location":"windows/#changing-install-location","title":"Changing Install Location","text":"<p>To install the Ollama application in a location different than your home directory, start the installer with the following flag</p> <pre><code>OllamaSetup.exe /DIR=\"d:\\some\\location\"\n</code></pre>"},{"location":"windows/#changing-model-location","title":"Changing Model Location","text":"<p>To change where Ollama stores the downloaded models instead of using your home directory, set the environment variable <code>OLLAMA_MODELS</code> in your user account.</p> <ol> <li> <p>Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for environment variables.</p> </li> <li> <p>Click on Edit environment variables for your account.</p> </li> <li> <p>Edit or create a new variable for your user account for <code>OLLAMA_MODELS</code> where you want the models stored</p> </li> <li> <p>Click OK/Apply to save.</p> </li> </ol> <p>If Ollama is already running, Quit the tray application and relaunch it from the Start menu, or a new terminal started after you saved the environment variables.</p>"},{"location":"windows/#api-access","title":"API Access","text":"<p>Here's a quick example showing API access from <code>powershell</code></p> <pre><code>(Invoke-WebRequest -method POST -Body '{\"model\":\"llama3.2\", \"prompt\":\"Why is the sky blue?\", \"stream\": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json\n</code></pre>"},{"location":"windows/#troubleshooting","title":"Troubleshooting","text":"<p>Ollama on Windows stores files in a few different locations.  You can view them in the explorer window by hitting <code>&lt;Ctrl&gt;+R</code> and type in: - <code>explorer %LOCALAPPDATA%\\Ollama</code> contains logs, and downloaded updates     - app.log contains most resent logs from the GUI application     - server.log contains the most recent server logs     - upgrade.log contains log output for upgrades - <code>explorer %LOCALAPPDATA%\\Programs\\Ollama</code> contains the binaries (The installer adds this to your user PATH) - <code>explorer %HOMEPATH%\\.ollama</code> contains models and configuration</p>"},{"location":"windows/#uninstall","title":"Uninstall","text":"<p>The Ollama Windows installer registers an Uninstaller application.  Under <code>Add or remove programs</code> in Windows Settings, you can uninstall Ollama.</p> <p>Note</p> <p>If you have changed the OLLAMA_MODELS location, the installer will not remove your downloaded models</p>"},{"location":"windows/#standalone-cli","title":"Standalone CLI","text":"<p>The easiest way to install Ollama on Windows is to use the <code>OllamaSetup.exe</code> installer. It installs in your account without requiring Administrator rights. We update Ollama regularly to support the latest models, and this installer will help you keep up to date.</p> <p>If you'd like to install or integrate Ollama as a service, a standalone <code>ollama-windows-amd64.zip</code> zip file is available containing only the Ollama CLI and GPU library dependencies for Nvidia.  If you have an AMD GPU, also download and extract the additional ROCm package <code>ollama-windows-amd64-rocm.zip</code> into the same directory.  This allows for embedding Ollama in existing applications, or running it as a system service via <code>ollama serve</code> with tools such as NSSM. </p> <p>Note</p> <p>If you are upgrading from a prior version, you should remove the old directories first.</p>"}]}